#!/usr/bin/env python3
"""
Script de production pour traiter TOUS les datasets en parallÃ¨le
CrÃ©e les 7 datasets sÃ©parÃ©s sur HuggingFace

Supports multi-resolution image generation for DeepSeek OCR training.
"""
import os
import sys
import argparse
from pathlib import Path
from dotenv import load_dotenv

from deepsynth.data.transforms.text_to_image import DEEPSEEK_OCR_RESOLUTIONS

# Charger .env
env_path = Path(__file__).parent / '.env'
load_dotenv(env_path)

def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="Process all datasets with multi-resolution image generation (always enabled)"
    )
    parser.add_argument(
        '--single-resolution',
        action='store_true',
        help='Generate ONLY single resolution images (disables multi-resolution)'
    )
    parser.add_argument(
        '--resolution-sizes',
        nargs='+',
        choices=list(DEEPSEEK_OCR_RESOLUTIONS.keys()),
        help='Specific resolution sizes to generate (default: all)'
    )
    parser.add_argument(
        '--max-workers',
        type=int,
        default=7,
        help='Maximum number of parallel workers (default: 7)'
    )
    return parser.parse_args()

def main():
    # Parse arguments
    args = parse_args()

    # VÃ©rifier le token
    if not os.getenv('HF_TOKEN'):
        print("âŒ HF_TOKEN non trouvÃ© dans .env")
        sys.exit(1)

    print("ğŸŒ TRAITEMENT COMPLET DES 7 DATASETS MULTILINGUES")
    print("=" * 70)
    print(f"âœ… HF_TOKEN configurÃ©")
    print(f"âœ… HF_USERNAME: {os.getenv('HF_USERNAME')}")
    print(f"âœ… ARXIV_IMAGE_SAMPLES: {os.getenv('ARXIV_IMAGE_SAMPLES', '50000')}")

    # Importer le pipeline
    from deepsynth.pipelines.parallel import ParallelDatasetsPipeline

    # Configuration pour production COMPLÃˆTE
    print("\nğŸ“Š CONFIGURATION PRODUCTION")
    print("-" * 70)
    print(f"âš™ï¸  Nombre de workers: {args.max_workers} (parallÃ©lisme)")
    print("ğŸ“¦ Mode: Production complÃ¨te (tous les Ã©chantillons disponibles)")
    print("ğŸ”„ Reprise automatique si dataset existant dÃ©tectÃ©")
    print("ğŸ“¤ Upload automatique tous les 5000 Ã©chantillons")

    # Multi-resolution info (always enabled unless --single-resolution flag)
    multi_resolution = not args.single_resolution

    if multi_resolution:
        if args.resolution_sizes:
            sizes_str = ', '.join(args.resolution_sizes)
            print(f"ğŸ” Multi-rÃ©solution activÃ©e: {sizes_str}")
        else:
            all_sizes = "/".join(DEEPSEEK_OCR_RESOLUTIONS.keys())
            print(f"ğŸ” Multi-rÃ©solution activÃ©e: toutes les tailles ({all_sizes})")
    else:
        print("ğŸ“¸ Mode rÃ©solution unique (image standard)")

    pipeline = ParallelDatasetsPipeline(
        max_workers=args.max_workers,
        multi_resolution=multi_resolution,
        resolution_sizes=args.resolution_sizes
    )

    # Afficher les datasets Ã  traiter
    print(f"\nğŸ“‹ DATASETS Ã€ TRAITER ({len(pipeline.datasets_config)} au total)")
    print("-" * 70)
    total_estimated = 0
    for i, dataset in enumerate(pipeline.datasets_config, 1):
        max_samples = dataset.get('max_samples')
        if max_samples:
            samples_info = f"max {max_samples:,} Ã©chantillons"
            total_estimated += max_samples
        else:
            samples_info = "complet (non limitÃ©)"

        priority_emoji = "ğŸ¥‡" if dataset['priority'] == 1 else "ğŸ¥ˆ" if dataset['priority'] == 2 else "ğŸ¥‰" if dataset['priority'] == 3 else "ğŸ“Š"
        print(f"{priority_emoji} {i}. {dataset['name']:<20} â†’ {dataset['output_name']:<25} ({samples_info})")

    print(f"\nğŸ“Š Total estimÃ©: ~{total_estimated:,}+ Ã©chantillons Ã  traiter")
    print("â±ï¸  Temps estimÃ©: 6-12 heures (selon matÃ©riel et connexion)")
    print("ğŸ’¾ Espace disque requis: ~15GB temporaire")

    print("\nâš ï¸  NOTES IMPORTANTES:")
    print("  â€¢ Le traitement peut Ãªtre interrompu (Ctrl+C) et reprendra automatiquement")
    print("  â€¢ Les datasets dÃ©jÃ  crÃ©Ã©s seront mis Ã  jour incrÃ©mentalement")
    print("  â€¢ Les logs dÃ©taillÃ©s sont dans 'parallel_datasets.log'")
    print("  â€¢ Chaque dataset sera visible sur HuggingFace dÃ¨s son upload")

    print("\n" + "=" * 70)

    try:
        print("ğŸš€ DÃ‰MARRAGE DU TRAITEMENT PARALLÃˆLE COMPLET")
        print("=" * 70)

        # Lancer le traitement de TOUS les datasets
        results = pipeline.run_parallel_processing()

        # RÃ©sumÃ© final
        successful = [r for r in results if r['status'] == 'success']
        failed = [r for r in results if r['status'] == 'error']

        print("\n" + "=" * 70)
        print("ğŸ‰ TRAITEMENT COMPLET TERMINÃ‰!")
        print("=" * 70)
        print(f"âœ… RÃ©ussis: {len(successful)}/{len(results)}")
        print(f"âŒ Ã‰chouÃ©s: {len(failed)}/{len(results)}")

        if successful:
            print(f"\nğŸ”— DATASETS CRÃ‰Ã‰S SUR HUGGINGFACE:")
            for result in successful:
                if 'repo_name' in result:
                    print(f"  ğŸ“¦ https://huggingface.co/datasets/{result['repo_name']}")

        if failed:
            print(f"\nâŒ DATASETS EN Ã‰CHEC:")
            for result in failed:
                print(f"  âš ï¸  {result['dataset']}: {result.get('error', 'Erreur inconnue')}")

        print(f"\nğŸ“‹ Consultez 'parallel_datasets.log' pour les dÃ©tails complets")

        return 0 if len(failed) == 0 else 1

    except KeyboardInterrupt:
        print("\nâ¸ï¸  TRAITEMENT INTERROMPU PAR L'UTILISATEUR")
        print("ğŸ’¡ Relancez ce script pour reprendre oÃ¹ vous vous Ãªtes arrÃªtÃ©")
        print("   Les datasets partiellement traitÃ©s seront complÃ©tÃ©s automatiquement")
        return 0

    except Exception as e:
        print(f"\nâŒ ERREUR CRITIQUE: {e}")
        import traceback
        traceback.print_exc()
        print("\nğŸ’¡ VÃ©rifiez 'parallel_datasets.log' pour plus de dÃ©tails")
        return 1

if __name__ == '__main__':
    sys.exit(main())

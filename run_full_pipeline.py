#!/usr/bin/env python3
"""
Script de production pour traiter TOUS les datasets en parallÃ¨le
CrÃ©e les 7 datasets sÃ©parÃ©s sur HuggingFace
"""
import os
import sys
from pathlib import Path
from dotenv import load_dotenv

# Charger .env
env_path = Path(__file__).parent / '.env'
load_dotenv(env_path)

def main():
    # VÃ©rifier le token
    if not os.getenv('HF_TOKEN'):
        print("âŒ HF_TOKEN non trouvÃ© dans .env")
        sys.exit(1)

    print("ğŸŒ TRAITEMENT COMPLET DES 7 DATASETS MULTILINGUES")
    print("=" * 70)
    print(f"âœ… HF_TOKEN configurÃ©")
    print(f"âœ… HF_USERNAME: {os.getenv('HF_USERNAME')}")
    print(f"âœ… ARXIV_IMAGE_SAMPLES: {os.getenv('ARXIV_IMAGE_SAMPLES', '50000')}")

    # Importer le pipeline
    from deepsynth.pipelines.parallel import ParallelDatasetsPipeline

    # Configuration pour production COMPLÃˆTE
    print("\nğŸ“Š CONFIGURATION PRODUCTION")
    print("-" * 70)
    print("âš™ï¸  Nombre de workers: 7 (1 par dataset, parallÃ©lisme maximal)")
    print("ğŸ“¦ Mode: Production complÃ¨te (tous les Ã©chantillons disponibles)")
    print("ğŸ”„ Reprise automatique si dataset existant dÃ©tectÃ©")
    print("ğŸ“¤ Upload automatique tous les 5000 Ã©chantillons")

    pipeline = ParallelDatasetsPipeline(max_workers=7)

    # Afficher les datasets Ã  traiter
    print(f"\nğŸ“‹ DATASETS Ã€ TRAITER ({len(pipeline.datasets_config)} au total)")
    print("-" * 70)
    total_estimated = 0
    for i, dataset in enumerate(pipeline.datasets_config, 1):
        max_samples = dataset.get('max_samples')
        if max_samples:
            samples_info = f"max {max_samples:,} Ã©chantillons"
            total_estimated += max_samples
        else:
            samples_info = "complet (non limitÃ©)"

        priority_emoji = "ğŸ¥‡" if dataset['priority'] == 1 else "ğŸ¥ˆ" if dataset['priority'] == 2 else "ğŸ¥‰" if dataset['priority'] == 3 else "ğŸ“Š"
        print(f"{priority_emoji} {i}. {dataset['name']:<20} â†’ {dataset['output_name']:<25} ({samples_info})")

    print(f"\nğŸ“Š Total estimÃ©: ~{total_estimated:,}+ Ã©chantillons Ã  traiter")
    print("â±ï¸  Temps estimÃ©: 6-12 heures (selon matÃ©riel et connexion)")
    print("ğŸ’¾ Espace disque requis: ~15GB temporaire")

    print("\nâš ï¸  NOTES IMPORTANTES:")
    print("  â€¢ Le traitement peut Ãªtre interrompu (Ctrl+C) et reprendra automatiquement")
    print("  â€¢ Les datasets dÃ©jÃ  crÃ©Ã©s seront mis Ã  jour incrÃ©mentalement")
    print("  â€¢ Les logs dÃ©taillÃ©s sont dans 'parallel_datasets.log'")
    print("  â€¢ Chaque dataset sera visible sur HuggingFace dÃ¨s son upload")

    print("\n" + "=" * 70)

    try:
        print("ğŸš€ DÃ‰MARRAGE DU TRAITEMENT PARALLÃˆLE COMPLET")
        print("=" * 70)

        # Lancer le traitement de TOUS les datasets
        results = pipeline.run_parallel_processing()

        # RÃ©sumÃ© final
        successful = [r for r in results if r['status'] == 'success']
        failed = [r for r in results if r['status'] == 'error']

        print("\n" + "=" * 70)
        print("ğŸ‰ TRAITEMENT COMPLET TERMINÃ‰!")
        print("=" * 70)
        print(f"âœ… RÃ©ussis: {len(successful)}/{len(results)}")
        print(f"âŒ Ã‰chouÃ©s: {len(failed)}/{len(results)}")

        if successful:
            print(f"\nğŸ”— DATASETS CRÃ‰Ã‰S SUR HUGGINGFACE:")
            for result in successful:
                if 'repo_name' in result:
                    print(f"  ğŸ“¦ https://huggingface.co/datasets/{result['repo_name']}")

        if failed:
            print(f"\nâŒ DATASETS EN Ã‰CHEC:")
            for result in failed:
                print(f"  âš ï¸  {result['dataset']}: {result.get('error', 'Erreur inconnue')}")

        print(f"\nğŸ“‹ Consultez 'parallel_datasets.log' pour les dÃ©tails complets")

        return 0 if len(failed) == 0 else 1

    except KeyboardInterrupt:
        print("\nâ¸ï¸  TRAITEMENT INTERROMPU PAR L'UTILISATEUR")
        print("ğŸ’¡ Relancez ce script pour reprendre oÃ¹ vous vous Ãªtes arrÃªtÃ©")
        print("   Les datasets partiellement traitÃ©s seront complÃ©tÃ©s automatiquement")
        return 0

    except Exception as e:
        print(f"\nâŒ ERREUR CRITIQUE: {e}")
        import traceback
        traceback.print_exc()
        print("\nğŸ’¡ VÃ©rifiez 'parallel_datasets.log' pour plus de dÃ©tails")
        return 1

if __name__ == '__main__':
    sys.exit(main())

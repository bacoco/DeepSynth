# ğŸš€ DeepSynth Vision-Language Summarization & RAG Platform

> Multilingual document understanding with parallel dataset generation, Unsloth-optimised fine-tuning, retrieval, and production-ready inference.

[![Production Ready](https://img.shields.io/badge/production-ready-green.svg)](docs/PRODUCTION_GUIDE.md)
[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)

DeepSynth turns large multilingual document collections into concise summaries using a vision-language stack derived from DeepSeek-OCR. The project couples high-throughput dataset preparation, Unsloth-powered training, retrieval-augmented generation, and multiple deployment surfaces (REST API, CLI, web UI, Docker).

---

## ğŸ”¥ Highlights

- **Parallel multilingual dataset builder** â€“ generate seven Hugging Face datasets (CNN/DailyMail, XSum, arXiv, BillSum, MLSUM FR/ES/DE) with resumable automation and logging via `scripts/generate_all_datasets.sh`.ã€F:scripts/generate_all_datasets.shâ€ L1-L118ã€‘
- **Unsloth fine-tuning CLI** â€“ `scripts/train_unsloth_cli.py` exposes end-to-end DeepSeek OCR training with QLoRA, WandB/TensorBoard hooks, checkpointing, and multi-backend dataset loaders.ã€F:scripts/train_unsloth_cli.pyâ€ L1-L146ã€‘
- **Retrieval-augmented inference** â€“ the `deepsynth.rag` package ingests encoded document states, performs multi-vector search, and fuses answers for advanced QA workflows.ã€F:src/deepsynth/rag/pipeline.pyâ€ L1-L172ã€‘
- **Production services** â€“ run summarisation through a Flask REST API or the configurable web dashboard (`python -m src.apps.web`).ã€F:src/deepsynth/inference/api_server.pyâ€ L1-L98ã€‘ã€F:src/apps/web/__main__.pyâ€ L1-L15ã€‘

---

## âš™ï¸ Getting started

### 1. Clone and create an environment
```bash
python3.12 -m venv .venv
source .venv/bin/activate
pip install --upgrade pip
pip install -r requirements-base.txt
pip install -r requirements-training.txt
```
> Or run `make setup` to execute the scripted installation (installs fonts and optional CUDA wheels).

### 2. Configure Hugging Face access
```bash
cp .env.example .env
# edit .env to add HF_TOKEN, HF_USERNAME, and dataset limits
```

### 3. Quick validation (optional)
```bash
make test-quick
```
This runs the fast pytest suite with `PYTHONPATH=./src` as defined in the Makefile.ã€F:Makefileâ€ L12-L40ã€‘

---

## ğŸ§± Core workflows

### Multilingual dataset generation
Use the orchestration script to prepare and upload all datasets in parallel:
```bash
./scripts/generate_all_datasets.sh
```
It validates your `.env`, boots the virtualenv if needed, cleans temporary directories, and runs the full pipeline (7 workers, resumable uploads, ~1.29M samples).ã€F:scripts/generate_all_datasets.shâ€ L1-L120ã€‘

### Fine-tuning with Unsloth optimisations
Launch Unsloth-enhanced training directly from the CLI:
```bash
PYTHONPATH=./src python scripts/train_unsloth_cli.py \
    --dataset_name ccdv/cnn_dailymail \
    --batch_size 4 \
    --num_epochs 3 \
    --use_wandb \
    --output_dir ./output/cnn_dailymail
```
The CLI handles Hugging Face datasets or local Parquet/WebDataset sources, configures QLoRA by default, and supports smoke tests, experiment tracking, and hub uploads.ã€F:scripts/train_unsloth_cli.pyâ€ L1-L200ã€‘

### Retrieval-augmented answering
Combine the encoder, multi-vector index, and storage layers to ingest visual states and answer questions:
```python
from deepsynth.rag.pipeline import IngestChunk, RAGPipeline
# configure featurizer/index/storage, ingest chunks, then call answer_query()
```
`RAGPipeline` manages ingestion manifests, state storage, vector search, and response fusion for downstream QA tasks.ã€F:src/deepsynth/rag/pipeline.pyâ€ L1-L172ã€‘

### Serving summaries
- **REST API:**
  ```bash
  MODEL_PATH=./deepsynth-summarizer python -m deepsynth.inference.api_server
  ```
  Exposes `/health`, `/summarize/text`, `/summarize/file`, and `/summarize/image` endpoints with automatic model initialisation and payload validation.ã€F:src/deepsynth/inference/api_server.pyâ€ L1-L98ã€‘
- **Web UI:**
  ```bash
  python -m src.apps.web
  ```
  Launches the Flask interface (port 5000 by default) for configuring datasets, training jobs, and monitoring progress.ã€F:src/apps/web/__main__.pyâ€ L1-L15ã€‘
- **Docker:** GPU-enabled compose files live under `deploy/` for containerised workflows (`docker compose -f deploy/docker-compose.gpu.yml up`).

---

## ğŸ§  Architecture deep dive

DeepSynth couples a visual document pipeline with Unsloth-optimised training so the encoder/decoder split stays lightweight while preserving layout fidelity:

- **Document rendering pipeline** â€“ dataset builders convert raw text into PNGs on demand, attach image columns, and push resumable shards to the Hugging Face Hub for multi-language coverage.ã€F:src/deepsynth/data/prepare_and_publish.pyâ€ L1-L210ã€‘ã€F:docs/IMAGE_PIPELINE.mdâ€ L1-L85ã€‘
- **Frozen vision encoder + QLoRA decoder** â€“ training keeps the DeepSeek-OCR encoder frozen while fine-tuning the mixture-of-experts decoder with low-rank adapters exposed through the Unsloth trainer CLI.ã€F:scripts/train_unsloth_cli.pyâ€ L1-L200ã€‘ã€F:docs/deepseek_ocr_pipeline.mdâ€ L1-L120ã€‘
- **Pipeline orchestration** â€“ the `deepsynth.pipelines` package streams samples through shared workers, handles deduplication, and coordinates uploads so dataset generation, training, and evaluation can progress independently.ã€F:src/deepsynth/pipelines/_dataset_processor.pyâ€ L1-L180ã€‘ã€F:docs/architecture/STRUCTURE.mdâ€ L1-L88ã€‘

The architecture documentation under `docs/architecture/` expands on these components, including deployment topology and shared volumes for the Docker stacks.ã€F:docs/architecture/STRUCTURE.mdâ€ L1-L88ã€‘

---

## ğŸ–¥ï¸ Web UI overview

The bundled UI wraps the end-to-end workflow with job monitoring, preset hyperparameters, and environment-specific Docker targets:

- **Dedicated CPU/GPU stacks** â€“ `docker-compose.cpu.yml` focuses on dataset generation while `docker-compose.gpu.yml` runs the trainer with GPU scheduling; both surface status dashboards via the web UI.ã€F:docs/ENHANCED_UI_GUIDE.mdâ€ L9-L64ã€‘
- **End-to-end orchestration** â€“ tabs for benchmark seeding, custom dataset creation, training, and monitoring map directly to the automation scripts, including Hugging Face uploads and progress metrics.ã€F:docs/ENHANCED_UI_GUIDE.mdâ€ L66-L160ã€‘
- **Local development entry point** â€“ launch with `python -m src.apps.web` to access the same interface without Docker while reusing local credentials and datasets.ã€F:src/apps/web/__main__.pyâ€ L1-L15ã€‘

Refer to `docs/ENHANCED_UI_GUIDE.md` for screenshots, presets, and role-based runbooks that align the UI with production and smoke-test scenarios.ã€F:docs/ENHANCED_UI_GUIDE.mdâ€ L1-L160ã€‘

---

## ğŸ—ºï¸ Repository map
```
DeepSynth/
â”œâ”€â”€ README.md                # Project overview (this file)
â”œâ”€â”€ docs/                    # Comprehensive documentation index & guides
â”œâ”€â”€ scripts/                 # Automation (dataset generation, Unsloth training, maintenance)
â”œâ”€â”€ src/deepsynth/           # Python package (data, training, inference, rag, pipelines)
â”œâ”€â”€ src/apps/web/            # Flask-based management UI
â”œâ”€â”€ tests/                   # Pytest suites mirroring src/
â”œâ”€â”€ tools/                   # Validation and end-to-end orchestration helpers
â””â”€â”€ deploy/                  # Dockerfiles and compose stacks
```
Refer to `docs/PROJECT_STRUCTURE.md` for a full breakdown of modules and workflows.ã€F:docs/PROJECT_STRUCTURE.mdâ€ L1-L78ã€‘

---

## ğŸ“š Documentation & support
- Start with the [documentation index](docs/README.md) for quick-start guides, architecture notes, and reports.ã€F:docs/README.mdâ€ L1-L52ã€‘
- `make help` lists every convenience command for setup, pipelines, and Unsloth targets.ã€F:Makefileâ€ L1-L88ã€‘
- Report issues or feature requests through GitHub; secrets must remain in `.env` as outlined in the repository guidelines.ã€F:AGENTS.mdâ€ L1-L37ã€‘

Happy summarising! ğŸ‰

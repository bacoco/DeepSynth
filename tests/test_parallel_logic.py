#!/usr/bin/env python3
"""
Test de la logique de parallÃ©lisation
Teste avec un petit nombre d'Ã©chantillons pour vÃ©rifier que tout fonctionne
"""

import os
import sys
import time
import logging

from deepsynth.pipelines.parallel_processing.parallel_datasets_builder import (
    ParallelDatasetsBuilder,
)

# Configuration du logging pour le test
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def test_parallel_logic():
    """Test de base de la logique parallÃ¨le avec limite de 500 Ã©chantillons"""
    print("ğŸ§ª TEST DE LA LOGIQUE DE PARALLÃ‰LISATION")
    print("="*50)

    # CrÃ©er le builder avec moins de workers pour le test
    builder = ParallelDatasetsBuilder(max_workers=2)

    # Modifier la configuration pour limiter Ã  500 Ã©chantillons par dataset pour le test
    for dataset_config in builder.datasets_config:
        dataset_config['max_samples'] = 500  # Limite stricte pour les tests

    # Tester avec seulement 2 datasets pour commencer
    test_datasets = ['CNN/DailyMail', 'XSum BBC']

    print(f"ğŸ“‹ Test avec {len(test_datasets)} datasets (max 500 Ã©chantillons chacun):")
    for dataset in test_datasets:
        print(f"  - {dataset}")

    print("\nğŸš€ DÃ©marrage du test parallÃ¨le...")
    start_time = time.time()

    try:
        # Lancer le traitement parallÃ¨le
        results = builder.run_parallel_processing(
            selected_datasets=test_datasets
        )

        duration = time.time() - start_time

        print(f"\nâœ… Test terminÃ© en {duration:.1f} secondes")
        print(f"ğŸ“Š RÃ©sultats: {len(results)} datasets traitÃ©s")

        # Analyser les rÃ©sultats
        successful = [r for r in results if r['status'] in ['success', 'already_complete']]
        failed = [r for r in results if r['status'] == 'error']

        print(f"âœ… SuccÃ¨s/Complets: {len(successful)}")
        print(f"âŒ Ã‰checs: {len(failed)}")

        if failed:
            print("\nâŒ ERREURS DÃ‰TECTÃ‰ES:")
            for result in failed:
                print(f"  - {result['dataset']}: {result.get('error', 'Erreur inconnue')}")

        return len(failed) == 0

    except Exception as e:
        print(f"\nâŒ ERREUR CRITIQUE: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def test_dataset_independence():
    """Test que les datasets sont bien indÃ©pendants"""
    print("\nğŸ” TEST D'INDÃ‰PENDANCE DES DATASETS")
    print("="*40)

    builder = ParallelDatasetsBuilder(max_workers=1)

    # VÃ©rifier que chaque dataset a un nom de sortie unique
    output_names = [d['output_name'] for d in builder.datasets_config]
    unique_names = set(output_names)

    if len(output_names) == len(unique_names):
        print("âœ… Tous les datasets ont des noms de sortie uniques")
        return True
    else:
        print("âŒ Conflit de noms de sortie dÃ©tectÃ©!")
        duplicates = [name for name in output_names if output_names.count(name) > 1]
        print(f"Doublons: {set(duplicates)}")
        return False

def main():
    """Point d'entrÃ©e principal du test"""
    print("ğŸ§ª SUITE DE TESTS POUR LA PARALLÃ‰LISATION")
    print("="*60)

    all_passed = True

    # Test 1: IndÃ©pendance des datasets
    print("\n1ï¸âƒ£ Test d'indÃ©pendance des datasets...")
    if not test_dataset_independence():
        all_passed = False

    # Test 2: Logique de parallÃ©lisation
    print("\n2ï¸âƒ£ Test de la logique de parallÃ©lisation...")
    if not test_parallel_logic():
        all_passed = False

    # RÃ©sumÃ© final
    print("\n" + "="*60)
    if all_passed:
        print("ğŸ‰ TOUS LES TESTS SONT PASSÃ‰S!")
        print("âœ… La parallÃ©lisation est prÃªte Ã  Ãªtre utilisÃ©e")
    else:
        print("âŒ CERTAINS TESTS ONT Ã‰CHOUÃ‰")
        print("ğŸ”§ VÃ©rifiez les erreurs ci-dessus avant de continuer")

    print("="*60)
    return all_passed

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)

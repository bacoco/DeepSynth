# Plan d'ImplÃ©mentation Final - Q&A Datasets avec Images PrÃ©-gÃ©nÃ©rÃ©es

## ğŸ“‹ Contexte et DÃ©cisions

### ProblÃ¨me Initial
- Documents Natural Questions trÃ¨s longs (moyenne 8,907 tokens)
- Extraction contextuelle faite Ã  la gÃ©nÃ©ration du dataset
- QualitÃ© calculÃ©e sans connaÃ®tre la rÃ©solution de training
- DÃ©cisions prises au mauvais moment (gÃ©nÃ©ration vs training)

### DÃ©cisions Finales

**DÃ‰CISION 1: Stocker document COMPLET dans le dataset**
- Raison: FlexibilitÃ© maximale, pas de perte d'information
- Le texte complet permet extraction contextuelle adaptative au training

**DÃ‰CISION 2: PRÃ‰-GÃ‰NÃ‰RER les images Ã  rÃ©solution MAXIMALE (gundam = 1600px)**
- Raison: Stockage HuggingFace gÃ©rÃ©, pas de contrainte d'espace
- Permet downscale vers tiny/base sans perte de qualitÃ©
- Training 0% overhead (pas de gÃ©nÃ©ration Ã  la volÃ©e)
- Meilleure qualitÃ© possible pour tous les cas d'usage

**DÃ‰CISION 3: Extraction contextuelle INTELLIGENTE Ã  la gÃ©nÃ©ration**
- BasÃ©e sur rÃ©solution cible (gundam = 1600px)
- Si document > capacitÃ© rÃ©solution â†’ extraction contextuelle
- Si document â‰¤ capacitÃ© rÃ©solution â†’ document complet
- Seuil dynamique: `max(2000, context_window * 2)`

**DÃ‰CISION 4: PrioritÃ© LONG answer**
- Long answer contient 15-142x plus de contexte
- Meilleur pour training du modÃ¨le
- Fallback sur short answer si pas de long

**DÃ‰CISION 5: Supprimer FiQA**
- Dataset sans vraies rÃ©ponses
- Inutilisable pour training

---

## ğŸ¯ Architecture Finale

### Flux de DonnÃ©es

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GÃ‰NÃ‰RATION DATASET (une fois)                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Natural Questions Sample                                        â”‚
â”‚  â”œâ”€ Document: 8,907 tokens (complet)                           â”‚
â”‚  â”œâ”€ Question: "when is the last episode..."                    â”‚
â”‚  â”œâ”€ Short answer: "March 18, 2018"                             â”‚
â”‚  â”œâ”€ Long answer: "The eighth season premiered..." (1,424 tok)  â”‚
â”‚  â””â”€ Answer positions: start=2114, end=3538                     â”‚
â”‚                                                                  â”‚
â”‚  â–¼ DÃ‰CISION: Doc trop long pour gundam (1600px)                â”‚
â”‚                                                                  â”‚
â”‚  Extraction Contextuelle                                        â”‚
â”‚  â”œâ”€ Context window: 800 tokens (gundam optimal)                â”‚
â”‚  â”œâ”€ Extraction: tokens[2114-800 : 3538+800]                   â”‚
â”‚  â””â”€ RÃ©sultat: ~2,224 tokens (contexte pertinent)              â”‚
â”‚                                                                  â”‚
â”‚  â–¼ GÃ‰NÃ‰RATION IMAGE                                             â”‚
â”‚                                                                  â”‚
â”‚  TextToImageConverter                                           â”‚
â”‚  â”œâ”€ RÃ©solution: gundam (1600Ã—1600)                             â”‚
â”‚  â”œâ”€ Input: 2,224 tokens                                        â”‚
â”‚  â””â”€ Output: Image 1600Ã—2224px (excellent quality)             â”‚
â”‚                                                                  â”‚
â”‚  â–¼ STOCKAGE HUGGINGFACE                                         â”‚
â”‚                                                                  â”‚
â”‚  {                                                              â”‚
â”‚    "text": "DOCUMENT COMPLET 8,907 tokens",                    â”‚
â”‚    "instruction": "when is the last episode...",               â”‚
â”‚    "answer": "The eighth season premiered..." (LONG priority)  â”‚
â”‚    "short_answer": "March 18, 2018",                           â”‚
â”‚    "long_answer": "The eighth season premiered...",            â”‚
â”‚    "answer_start_token": 2114,                                 â”‚
â”‚    "answer_end_token": 3538,                                   â”‚
â”‚    "image": <PIL.Image 1600Ã—2224>,  â† PRÃ‰-GÃ‰NÃ‰RÃ‰E             â”‚
â”‚    "quality": "excellent",                                      â”‚
â”‚    "estimated_height": 2224,                                   â”‚
â”‚    "token_count": 8907,                                        â”‚
â”‚    "extracted_token_count": 2224,                              â”‚
â”‚    "metadata": {                                               â”‚
â”‚      "extraction_method": "contextual",                        â”‚
â”‚      "context_window": 800,                                    â”‚
â”‚      "generation_resolution": "gundam",                        â”‚
â”‚      ...                                                       â”‚
â”‚    }                                                           â”‚
â”‚  }                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TRAINING (rÃ©pÃ©tÃ©)                                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  InstructionDataset.__getitem__(idx)                            â”‚
â”‚  â”œâ”€ Charger sample depuis HuggingFace                          â”‚
â”‚  â”œâ”€ RÃ©cupÃ©rer image PRÃ‰-GÃ‰NÃ‰RÃ‰E (1600Ã—2224)                   â”‚
â”‚  â”œâ”€ Appliquer transform si nÃ©cessaire (resize/augmentation)    â”‚
â”‚  â”‚   - Training en "tiny" â†’ resize 1600Ã—2224 â†’ 512Ã—512        â”‚
â”‚  â”‚   - Training en "base" â†’ resize 1600Ã—2224 â†’ 1024Ã—1024      â”‚
â”‚  â”‚   - Training en "gundam" â†’ utiliser tel quel (optimal!)    â”‚
â”‚  â””â”€ Return sample                                              â”‚
â”‚                                                                  â”‚
â”‚  â±ï¸  Temps: 0ms gÃ©nÃ©ration (image dÃ©jÃ  prÃªte!)                  â”‚
â”‚  ğŸ“Š Overhead: 0%                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Changements DÃ©taillÃ©s par Fichier

### 1. `src/deepsynth/data/quality_calculator.py`

**Ajouter fonction pour calcul context window optimal**

```python
def calculate_optimal_context_window(
    target_resolution: str = "gundam",
    chars_per_line: int = 100,
    line_height_px: int = 20,
) -> int:
    """
    Calculate optimal context window for target resolution.

    Args:
        target_resolution: "tiny", "small", "base", "large", "gundam"
        chars_per_line: Characters per line (default: 100)
        line_height_px: Line height in pixels (default: 20)

    Returns:
        Optimal context window in tokens

    Example:
        >>> calculate_optimal_context_window("gundam")  # 1600px height
        800  # tokens (1600px / 20px/line * 100chars/line / 5chars/token)

        >>> calculate_optimal_context_window("base")  # 1024px height
        512  # tokens
    """
    from .transforms.text_to_image import DEEPSEEK_OCR_RESOLUTIONS

    if target_resolution not in DEEPSEEK_OCR_RESOLUTIONS:
        raise ValueError(f"Unknown resolution: {target_resolution}")

    target_height = DEEPSEEK_OCR_RESOLUTIONS[target_resolution][1]  # (width, height)

    # Calculate tokens that fit in target height
    lines_per_image = target_height / line_height_px
    chars_per_image = lines_per_image * chars_per_line
    tokens_per_image = chars_per_image / CHARS_PER_TOKEN

    # Context window is half of capacity (for before + after)
    context_window = int(tokens_per_image / 2)

    return context_window
```

**Ajouter fonction pour dÃ©cision extraction**

```python
def should_extract_context(
    token_count: int,
    target_resolution: str = "gundam",
    min_threshold: int = 2000,
) -> tuple[bool, int]:
    """
    Decide if contextual extraction is needed based on document size and resolution.

    Args:
        token_count: Document token count
        target_resolution: Target resolution for image generation
        min_threshold: Minimum threshold before considering extraction

    Returns:
        Tuple of (should_extract, context_window_if_needed)

    Example:
        >>> should_extract_context(500, "gundam")
        (False, 0)  # Doc court, pas besoin d'extraction

        >>> should_extract_context(10000, "gundam")
        (True, 800)  # Doc long, extraction avec window=800
    """
    context_window = calculate_optimal_context_window(target_resolution)

    # Seuil dynamique: max(min_threshold, context_window * 2)
    extraction_threshold = max(min_threshold, context_window * 2)

    if token_count <= extraction_threshold:
        # Document assez court, pas besoin d'extraction
        return (False, 0)
    else:
        # Document trop long, extraction nÃ©cessaire
        return (True, context_window)
```

---

### 2. `src/deepsynth/data/dataset_converters/natural_questions.py`

**CHANGEMENTS MAJEURS**

**A. Modifier signature de `convert_natural_questions()`**

```python
def convert_natural_questions(
    split: str = "train",
    max_samples: Optional[int] = None,
    streaming: bool = True,
    target_resolution: str = "gundam",  # â† NOUVEAU: RÃ©solution pour prÃ©-gÃ©nÃ©ration
) -> InstructionDataset:
    """
    Convert Natural Questions to instruction format with pre-generated images.

    Args:
        split: Dataset split ("train", "validation")
        max_samples: Maximum number of samples (None = all)
        streaming: Use streaming mode
        target_resolution: Target resolution for pre-generation ("gundam" recommended)

    Returns:
        InstructionDataset with pre-generated images at target resolution
    """
```

**B. Calculer context window optimal**

```python
from ..quality_calculator import should_extract_context, calculate_optimal_context_window

# Au dÃ©but de la fonction
context_window = calculate_optimal_context_window(target_resolution)
LOGGER.info(f"Target resolution: {target_resolution}")
LOGGER.info(f"Optimal context window: {context_window} tokens")
```

**C. DÃ©cision extraction intelligente**

```python
# AprÃ¨s avoir extrait short et long answers...

# Calculer longueur document complet
full_document_token_count = len(text_tokens)

# DÃ©cider stratÃ©gie d'extraction
should_extract, extraction_window = should_extract_context(
    full_document_token_count,
    target_resolution
)

if should_extract:
    # Document trop long â†’ extraction contextuelle
    if answer_start is not None and answer_end is not None:
        document_text = extract_contextual_window(
            text_tokens,
            answer_start,
            answer_end,
            context_before=extraction_window,
            context_after=extraction_window,
        )
        extraction_method = "contextual"
        extracted_token_count = len(document_text.split())
    else:
        # Cas rare: pas de positions, utiliser answer text
        document_text = primary_answer
        extraction_method = "answer_only"
        extracted_token_count = len(document_text.split())
else:
    # Document assez court â†’ garder ENTIER
    document_text = " ".join(str(t) for t in text_tokens)
    extraction_method = "full_document"
    extracted_token_count = full_document_token_count
```

**D. GÃ©nÃ©rer image Ã  rÃ©solution cible**

```python
from ..transforms.text_to_image import TextToImageConverter

# GÃ©nÃ©rer image Ã  rÃ©solution cible
converter = TextToImageConverter(max_width=1600, max_height=10000)  # Gundam width
image = converter.convert(document_text)

# Note: L'image sera automatiquement redimensionnÃ©e si trop grande
# mais avec max_height=10000, on peut gÃ©rer mÃªme les docs trÃ¨s longs
```

**E. Calculer quality indicators**

```python
# Calculer qualitÃ© basÃ©e sur document EXTRAIT (pas complet)
quality, quality_desc, estimated_height = calculate_quality(extracted_token_count)
```

**F. Stocker TOUTES les informations**

```python
converted_samples.append({
    # Texte et instruction
    "text": " ".join(str(t) for t in text_tokens),  # â† DOCUMENT COMPLET
    "instruction": question.strip(),

    # Answers (long priority)
    "answer": primary_answer.strip(),
    "short_answer": short_answer_text.strip(),
    "long_answer": long_answer_text.strip(),

    # Positions pour extraction future si besoin
    "answer_start_token": answer_start,  # â† NOUVEAU
    "answer_end_token": answer_end,      # â† NOUVEAU

    # Image prÃ©-gÃ©nÃ©rÃ©e
    "image": image,  # â† IMAGE PRÃ‰-GÃ‰NÃ‰RÃ‰E Ã€ RÃ‰SOLUTION CIBLE

    # Quality indicators
    "quality": quality,
    "estimated_height": estimated_height,
    "token_count": full_document_token_count,  # â† Document COMPLET
    "extracted_token_count": extracted_token_count,  # â† Document EXTRAIT (pour image)

    # Metadata Ã©tendu
    "metadata": {
        "source": "natural_questions",
        "original_index": idx,
        "answer_type": answer_type,
        "has_short": bool(short_answer_text),
        "has_long": bool(long_answer_text),
        "extraction_method": extraction_method,  # "contextual", "full_document", "answer_only"
        "context_window": extraction_window if should_extract else 0,
        "generation_resolution": target_resolution,  # â† NOUVEAU
        "quality_description": quality_desc,
    },
})
```

---

### 3. `src/deepsynth/data/dataset_converters/ms_marco.py`

**Changements similaires mais plus simples** (MS MARCO a dÃ©jÃ  des docs courts)

```python
def convert_ms_marco(
    config: str = "v2.1",
    split: str = "train",
    max_samples: Optional[int] = None,
    streaming: bool = True,
    target_resolution: str = "gundam",  # â† NOUVEAU
) -> InstructionDataset:
```

**GÃ©nÃ©ration d'image** (MS MARCO raremen besoin d'extraction):

```python
from ..transforms.text_to_image import TextToImageConverter

# MS MARCO a gÃ©nÃ©ralement des passages courts, pas besoin d'extraction
converter = TextToImageConverter(max_width=1600, max_height=10000)
image = converter.convert(document)

# Calculer quality
token_count = len(document.split())
quality, quality_desc, estimated_height = calculate_quality(token_count)

converted_samples.append({
    "text": document.strip(),
    "instruction": query.strip(),
    "answer": answer.strip(),
    "short_answer": answer.strip(),
    "long_answer": "",
    "answer_start_token": None,  # MS MARCO n'a pas de positions
    "answer_end_token": None,
    "image": image,  # â† IMAGE PRÃ‰-GÃ‰NÃ‰RÃ‰E
    "quality": quality,
    "estimated_height": estimated_height,
    "token_count": token_count,
    "extracted_token_count": token_count,  # MÃªme que token_count (pas d'extraction)
    "metadata": {
        "source": "ms_marco",
        "config": config,
        "original_index": idx,
        "has_short": True,
        "has_long": False,
        "answer_type": "short",
        "extraction_method": "full_document",
        "generation_resolution": target_resolution,
        "quality_description": quality_desc,
    },
})
```

---

### 4. `src/deepsynth/data/instruction_dataset.py`

**AUCUN changement majeur nÃ©cessaire !**

Le `__getitem__()` actuel gÃ¨re dÃ©jÃ  bien:

```python
def __getitem__(self, idx):
    sample = self.dataset[idx]

    # Si image existe (notre cas), la charger
    if "image" in sample and sample["image"] is not None:
        image = sample["image"]

        # Appliquer transform (resize si besoin)
        if self.transform is not None:
            image = self.transform(image)
```

**Optionnel: Ajouter logging pour debug**

```python
# Dans __init__()
if len(self.dataset) > 0:
    first_sample = self.dataset[0]
    if "generation_resolution" in first_sample.get("metadata", {}):
        gen_res = first_sample["metadata"]["generation_resolution"]
        LOGGER.info(f"Dataset generated at resolution: {gen_res}")
```

---

### 5. `src/deepsynth/data/dataset_converters/__init__.py`

**DÃ©jÃ  Ã  jour** (FiQA supprimÃ©)

Pas de changements nÃ©cessaires.

---

## ğŸ“ Algorithmes DÃ©taillÃ©s

### Algorithme 1: Calcul Context Window Optimal

```
INPUT: target_resolution (string: "tiny"|"small"|"base"|"large"|"gundam")
OUTPUT: context_window (int: nombre de tokens)

1. RÃ©cupÃ©rer hauteur cible:
   resolutions = {"tiny": 512, "small": 640, "base": 1024, "large": 1280, "gundam": 1600}
   target_height = resolutions[target_resolution]

2. Calculer capacitÃ© en tokens:
   lines_per_image = target_height / 20  (20px par ligne)
   chars_per_image = lines_per_image * 100  (100 chars par ligne)
   tokens_per_image = chars_per_image / 5  (5 chars par token)

3. Context window = moitiÃ© de la capacitÃ©:
   context_window = tokens_per_image / 2

EXEMPLE:
  target_resolution = "gundam" (1600px)
  â†’ lines = 1600/20 = 80 lignes
  â†’ chars = 80*100 = 8000 chars
  â†’ tokens = 8000/5 = 1600 tokens
  â†’ context_window = 1600/2 = 800 tokens
```

### Algorithme 2: DÃ©cision Extraction Contextuelle

```
INPUT:
  - document_token_count (int)
  - target_resolution (string)
  - min_threshold (int, default=2000)

OUTPUT: (should_extract: bool, context_window: int)

1. Calculer context window optimal:
   context_window = calculate_optimal_context_window(target_resolution)

2. Calculer seuil d'extraction:
   extraction_threshold = max(min_threshold, context_window * 2)

   Raison: Si doc â‰¤ 2*window, l'extraction contextuelle Â±window
           couvrirait presque tout le document de toute faÃ§on

3. DÃ©cision:
   if document_token_count <= extraction_threshold:
       return (False, 0)  // Garder document entier
   else:
       return (True, context_window)  // Extraction nÃ©cessaire

EXEMPLES:
  1. Doc 500 tokens, gundam (window=800)
     â†’ threshold = max(2000, 800*2) = 2000
     â†’ 500 â‰¤ 2000 â†’ (False, 0) â†’ Garder entier

  2. Doc 10000 tokens, gundam (window=800)
     â†’ threshold = max(2000, 800*2) = 2000
     â†’ 10000 > 2000 â†’ (True, 800) â†’ Extraction Â±800

  3. Doc 1500 tokens, tiny (window=256)
     â†’ threshold = max(2000, 256*2) = 2000
     â†’ 1500 â‰¤ 2000 â†’ (False, 0) â†’ Garder entier
```

### Algorithme 3: Extraction Contextuelle avec Positions

```
INPUT:
  - tokens (list[str]): All document tokens
  - answer_start (int): Start position of answer
  - answer_end (int): End position of answer
  - context_window (int): Tokens to extract before/after

OUTPUT: extracted_text (str)

1. Calculer bornes:
   start_idx = max(0, answer_start - context_window)
   end_idx = min(len(tokens), answer_end + context_window)

2. Extraire:
   context_tokens = tokens[start_idx:end_idx]

3. Joindre:
   extracted_text = " ".join(context_tokens)

4. Return extracted_text

EXEMPLE:
  tokens = ["The", "Walking", "Dead", ..., "March", "18", "2018", ...]  (10000 tokens)
  answer_start = 3521
  answer_end = 3525
  context_window = 800

  â†’ start_idx = max(0, 3521-800) = 2721
  â†’ end_idx = min(10000, 3525+800) = 4325
  â†’ context_tokens = tokens[2721:4325]  (1604 tokens)
  â†’ extracted_text = "... March 18 2018 ..."
```

---

## ğŸ§ª Tests de Validation

### Test 1: VÃ©rifier GÃ©nÃ©ration Image

```python
def test_image_generation_at_gundam():
    """Test que les images sont gÃ©nÃ©rÃ©es Ã  rÃ©solution gundam."""
    from deepsynth.data.dataset_converters import convert_natural_questions

    dataset = convert_natural_questions(
        split="train",
        max_samples=10,
        streaming=True,
        target_resolution="gundam"
    )

    for sample in dataset:
        assert "image" in sample
        assert sample["image"] is not None

        # VÃ©rifier rÃ©solution
        img = sample["image"]
        width, height = img.size
        assert width <= 1600, f"Width {width} > 1600"

        # VÃ©rifier metadata
        assert sample["metadata"]["generation_resolution"] == "gundam"

    print("âœ… Images gÃ©nÃ©rÃ©es Ã  rÃ©solution gundam")
```

### Test 2: VÃ©rifier Extraction Intelligente

```python
def test_intelligent_extraction():
    """Test que l'extraction se fait seulement si nÃ©cessaire."""
    from deepsynth.data.dataset_converters import convert_natural_questions

    dataset = convert_natural_questions(
        split="train",
        max_samples=100,
        streaming=True,
        target_resolution="gundam"
    )

    full_doc_count = 0
    contextual_count = 0

    for sample in dataset:
        method = sample["metadata"]["extraction_method"]
        token_count = sample["token_count"]
        extracted_count = sample["extracted_token_count"]

        if method == "full_document":
            full_doc_count += 1
            assert token_count == extracted_count, "Full doc should have same counts"
            assert token_count <= 2000, f"Full doc with {token_count} tokens should be extracted"

        elif method == "contextual":
            contextual_count += 1
            assert extracted_count < token_count, "Contextual should reduce token count"
            assert token_count > 2000, f"Doc with {token_count} tokens doesn't need extraction"

    print(f"âœ… Full documents: {full_doc_count}")
    print(f"âœ… Contextual extraction: {contextual_count}")
    assert full_doc_count > 0, "Should have some full documents"
    assert contextual_count > 0, "Should have some contextual extractions"
```

### Test 3: VÃ©rifier Positions StockÃ©es

```python
def test_answer_positions_stored():
    """Test que les positions des rÃ©ponses sont stockÃ©es."""
    from deepsynth.data.dataset_converters import convert_natural_questions

    dataset = convert_natural_questions(
        split="train",
        max_samples=10,
        streaming=True,
        target_resolution="gundam"
    )

    for sample in dataset:
        if sample["metadata"]["has_long"] or sample["metadata"]["has_short"]:
            assert "answer_start_token" in sample
            assert "answer_end_token" in sample
            assert sample["answer_start_token"] is not None
            assert sample["answer_end_token"] is not None

    print("âœ… Answer positions stored correctly")
```

### Test 4: VÃ©rifier Document Complet StockÃ©

```python
def test_full_document_stored():
    """Test que le document COMPLET est stockÃ© dans 'text'."""
    from deepsynth.data.dataset_converters import convert_natural_questions

    dataset = convert_natural_questions(
        split="train",
        max_samples=10,
        streaming=True,
        target_resolution="gundam"
    )

    for sample in dataset:
        text_tokens = len(sample["text"].split())
        stored_count = sample["token_count"]

        # Le nombre de tokens dans 'text' devrait matcher token_count
        # (avec petite marge pour variations de tokenization)
        assert abs(text_tokens - stored_count) < 100, \
            f"Text has {text_tokens} tokens but token_count is {stored_count}"

        # Si extraction contextuelle, extracted_token_count devrait Ãªtre < token_count
        if sample["metadata"]["extraction_method"] == "contextual":
            assert sample["extracted_token_count"] < sample["token_count"]

    print("âœ… Full document stored in 'text' field")
```

### Test 5: Benchmark Temps de Loading

```python
def test_loading_speed():
    """Test que le chargement est rapide (images prÃ©-gÃ©nÃ©rÃ©es)."""
    import time
    from deepsynth.data.dataset_converters import convert_natural_questions
    from deepsynth.data.instruction_dataset import InstructionDataset

    # Convertir dataset
    raw_dataset = convert_natural_questions(
        split="train",
        max_samples=100,
        streaming=True,
        target_resolution="gundam"
    )

    # CrÃ©er InstructionDataset
    dataset = InstructionDataset(raw_dataset)

    # Mesurer temps de chargement
    start = time.time()
    for i in range(10):
        sample = dataset[i]
        assert sample["image"] is not None
    end = time.time()

    avg_time = (end - start) / 10 * 1000  # ms

    print(f"âœ… Average loading time: {avg_time:.1f}ms per sample")
    assert avg_time < 50, f"Loading too slow: {avg_time}ms (should be <50ms)"
```

---

## âœ… CritÃ¨res d'Acceptation

### Fonctionnel

- [ ] Images prÃ©-gÃ©nÃ©rÃ©es Ã  rÃ©solution gundam (1600px)
- [ ] Extraction contextuelle intelligente (seulement si doc > 2000 tokens)
- [ ] Document COMPLET stockÃ© dans field "text"
- [ ] Positions answer stockÃ©es (answer_start_token, answer_end_token)
- [ ] Long answer en prioritÃ©
- [ ] Metadata complet avec generation_resolution
- [ ] MS MARCO Ã©galement avec images prÃ©-gÃ©nÃ©rÃ©es
- [ ] FiQA supprimÃ©

### Performance

- [ ] Chargement image <50ms par sample
- [ ] Training overhead 0% (pas de gÃ©nÃ©ration Ã  la volÃ©e)
- [ ] Dataset size acceptable sur HuggingFace

### QualitÃ©

- [ ] Natural Questions: 40% skip rate (normal - pas de rÃ©ponse)
- [ ] Quality distribution: 80%+ excellent/good
- [ ] Images lisibles Ã  rÃ©solution gundam
- [ ] Downscale vers tiny/base fonctionne

### Tests

- [ ] Tous les tests unitaires passent
- [ ] Test intÃ©gration gÃ©nÃ©ration dataset
- [ ] Test training avec dataset gÃ©nÃ©rÃ©
- [ ] Validation qualitÃ© visuelle des images

---

## ğŸ“Š Estimation Taille Dataset

### Natural Questions (300k samples)

**Avec extraction contextuelle** (docs rÃ©duits Ã  ~1500 tokens avg):
- Image size: 1600px Ã— 1500px Ã— 3 (RGB) â‰ˆ 7.2 MB par sample
- Total: 300,000 Ã— 7.2 MB â‰ˆ **2.16 TB**

**Compression HuggingFace** (PNG avec compression):
- Facteur compression: ~5-10x
- Total compressÃ©: **216-432 GB**

### MS MARCO (1M samples)

**Documents courts** (~200 tokens avg):
- Image size: 1600px Ã— 200px Ã— 3 â‰ˆ 960 KB par sample
- Total: 1,000,000 Ã— 960 KB â‰ˆ **960 GB**
- CompressÃ©: **96-192 GB**

### Total EstimÃ©

- Natural Questions: **216-432 GB**
- MS MARCO: **96-192 GB**
- **TOTAL: 312-624 GB** sur HuggingFace

C'est gÃ©rable pour HuggingFace (ils supportent des datasets bien plus larges).

---

## ğŸš€ Ordre d'ImplÃ©mentation RecommandÃ©

### Phase 1: Fonctions Utilitaires (1-2h)
1. `quality_calculator.py`:
   - `calculate_optimal_context_window()`
   - `should_extract_context()`

### Phase 2: Natural Questions (2-3h)
1. Modifier `convert_natural_questions()`:
   - Ajouter paramÃ¨tre `target_resolution`
   - Calcul context window
   - DÃ©cision extraction intelligente
   - GÃ©nÃ©rer image
   - Stocker positions + document complet

### Phase 3: MS MARCO (1h)
1. Modifier `convert_ms_marco()`:
   - Ajouter paramÃ¨tre `target_resolution`
   - GÃ©nÃ©rer image
   - Metadata

### Phase 4: Tests (1-2h)
1. Tests unitaires
2. Tests intÃ©gration
3. Validation visuelle

### Phase 5: Documentation (30min)
1. Update README
2. Update tests docs

**TOTAL ESTIMÃ‰: 6-9 heures de dÃ©veloppement**

---

## ğŸ’¡ Notes pour le DÃ©veloppeur

### PiÃ¨ges Ã  Ã‰viter

1. **Ne PAS utiliser `[:1000]` pour tronquer le texte**
   - Utiliser `should_extract_context()` pour dÃ©cider
   - Utiliser `extract_contextual_window()` pour extraire proprement

2. **Stocker le document COMPLET dans "text"**
   - Pas le document extrait
   - L'extraction est seulement pour l'image

3. **Positions en tokens, pas en caractÃ¨res**
   - `answer_start_token` et `answer_end_token`
   - Pas `answer_start_char`

4. **Image resolution = gundam (1600px width)**
   - Pas 1024 (base) ou 512 (tiny)
   - gundam = meilleure qualitÃ©

### Optimisations Futures

- Caching des images gÃ©nÃ©rÃ©es en local pour dev
- GÃ©nÃ©ration parallÃ¨le (multiprocessing)
- Validation qualitÃ© automatique des images
- Support multi-rÃ©solution (stocker tiny + gundam)

---

## ğŸ“ Questions / Clarifications

Si des questions pendant l'implÃ©mentation, vÃ©rifier:
1. Ce document en premier
2. Code existant dans `natural_questions.py` (pour patterns)
3. Tests existants (pour exemples)

**Contact**: [Ton contact ici]

---

**Document Version**: 1.0
**Date**: 2025-10-28
**ApprouvÃ© par**: Loic
**RÃ©solution cible**: gundam (1600px)

# ðŸ§± DeepSynth Repository Structure & Dependencies

This document summarises the updated layout after consolidating documentation into `docs/` and deployment tooling into `deploy/`. Use it to orient yourself quickly before diving into a feature or a refactor.

## ðŸ“‚ Top-level layout

```
DeepSynth/
â”œâ”€â”€ deploy/
â”‚   â”œâ”€â”€ Dockerfile             # CPU/GPU base images for the services
â”‚   â”œâ”€â”€ Dockerfile.cpu
â”‚   â”œâ”€â”€ docker-compose.yml     # Full stack (dataset + trainer)
â”‚   â”œâ”€â”€ docker-compose.cpu.yml # CPU-only dataset generator
â”‚   â”œâ”€â”€ docker-compose.gpu.yml # GPU-accelerated trainer
â”‚   â”œâ”€â”€ deployment-api-docs.md # REST API & rollout guidance
â”‚   â”œâ”€â”€ start-all.sh           # Orchestrates CPU + GPU stacks
â”‚   â”œâ”€â”€ start-dataset-generation.sh
â”‚   â”œâ”€â”€ start-model-training.sh
â”‚   â””â”€â”€ start_docker_ui.sh
â”œâ”€â”€ docs/                      # All markdown documentation (this folder)
â”‚   â”œâ”€â”€ README.md              # Documentation index
â”‚   â”œâ”€â”€ architecture/          # Structural views & dependency notes
â”‚   â”œâ”€â”€ *.md                   # Guides, delivery notes, reportsâ€¦
â”œâ”€â”€ data/                      # Dataset extraction and preprocessing helpers
â”œâ”€â”€ evaluation/                # Benchmarking harness & report generation
â”œâ”€â”€ inference/                 # Inference server, CLI, and utilities
â”œâ”€â”€ parallel_processing/       # Concurrency primitives and README
â”œâ”€â”€ scripts/                   # Automation scripts (Python) used across pipelines
â”œâ”€â”€ training/                  # Training loops, configs, and utilities
â”œâ”€â”€ web_ui/                    # Front-end assets for the dataset/model UI
â””â”€â”€ tests/                     # Automated regression suites
```

## ðŸ”— Dependency highlights

| Area | Key dependencies | Notes |
| --- | --- | --- |
| `deploy/` | Docker, Docker Compose, optional NVIDIA runtime | Shell scripts now resolve paths relative to `deploy/`, so they can be executed from the repo root without manual `cd`. |
| Data pipelines (`data/`, `scripts/`, `run_*`) | `datasets`, `huggingface_hub`, Pillow | Responsible for dataset ingestion, text-to-image conversion, and incremental uploads. |
| Training (`training/`, `run_*training*.sh`) | PyTorch, Accelerate | Consumes datasets generated by the pipelines; GPU optional but recommended. |
| Evaluation (`evaluation/`, `tests/`) | `rouge-score`, `nltk`, project modules | Benchmarks fine-tuned models and validates dataset integrity. |
| Web UI (`web_ui/`, `deploy/docker-compose*.yml`) | Flask, React (bundled), Redis | Served through the Docker stack; shares volumes (`generated_images/`, `trained_model/`, `logs/`). |
| SOUL memory (`docs/.agent_*`, `docs/SOUL_README.md`) | Git tracked markdown/json | Acts as the coordination fabric across agents and contributors. |

## ðŸ“Ž Cross-cutting concerns

- **Shared volumes** â€“ `generated_images/`, `trained_model/`, and `logs/` are created automatically by `deploy/start_docker_ui.sh` so that containers share state with local tooling.
- **Environment variables** â€“ `.env` (kept at repo root) powers both Compose stacks and Python scripts. Deployment scripts expect it adjacent to the repository root.
- **Documentation flow** â€“ Start with `docs/README.md` for navigation, then deep-dive into the specific guide (e.g., `docs/PRODUCTION_GUIDE.md`) or cross-reference architecture notes here.
- **Testing discipline** â€“ Unit tests live in `tests/` while integration smoke tests are described inside `docs/VERIFICATION_REPORT.md`. Keep both updated when changing core pipelines.

## âœ… Next steps for new contributors

1. Read `docs/README.md` to choose the relevant guide for your task.
2. Provision the environment with `setup.sh` and, if needed, build containers via `deploy/start-all.sh`.
3. Capture your progress in `docs/.agent_log.md` and hand off actionable next steps in `docs/.agent_handoff.md`.

#!/bin/bash
#
# Script de lancement pour g√©n√©rer TOUS les datasets en parall√®le
# Usage: ./generate_all_datasets.sh
#
# Ce script va cr√©er les 7 datasets multilingues sur HuggingFace:
#   1. deepsynth-en-news   (CNN/DailyMail ~287k)
#   2. deepsynth-en-arxiv  (arXiv Scientific ~50k)
#   3. deepsynth-en-xsum   (BBC XSum ~50k)
#   4. deepsynth-fr        (MLSUM Fran√ßais ~392k)
#   5. deepsynth-es        (MLSUM Espagnol ~266k)
#   6. deepsynth-de        (MLSUM Allemand ~220k)
#   7. deepsynth-en-legal  (BillSum Legal ~22k)
#
# Chaque dataset contiendra des images originales haute qualit√©
#   Augmentation al√©atoire appliqu√©e pendant l'entra√Ænement:
#   - Rotation (¬±10¬∞), perspective, resize (512-1600px), color jitter
#
# Dur√©e estim√©e: 2-4 heures (6x plus rapide qu'avant!)
# Espace disque requis: ~2.5GB temporaire (6x moins qu'avant!)
#

set -e  # Arr√™t en cas d'erreur

echo "üåç DEEPSYNTH - G√©n√©ration de tous les datasets en parall√®le"
echo "============================================================="

# V√©rifier et installer les d√©pendances si n√©cessaire
if [ ! -d "venv" ] || [ ! -f "/Library/Fonts/DejaVuSans.ttf" ]; then
    echo "‚öôÔ∏è  Installation des d√©pendances et fonts Unicode..."
    echo "   (Premi√®re ex√©cution uniquement - peut demander sudo)"
    echo ""

    if [ ! -x "./setup.sh" ]; then
        chmod +x setup.sh
    fi

    ./setup.sh

    echo ""
    echo "‚úÖ Installation termin√©e"
    echo "============================================================="
    echo ""
fi

# Activer l'environnement virtuel
if [ -f "venv/bin/activate" ]; then
    source venv/bin/activate
    echo "‚úÖ Environnement virtuel activ√©"
else
    echo "‚ö†Ô∏è  Pas d'environnement virtuel d√©tect√©"
    echo "   Utilisation de Python syst√®me"
fi

echo ""

# V√©rifier que le fichier .env existe
if [ ! -f .env ]; then
    echo "‚ùå Erreur: Fichier .env introuvable"
    echo "üí° Copiez .env.example vers .env et configurez HF_TOKEN"
    exit 1
fi

# V√©rifier HF_TOKEN
if ! grep -q "HF_TOKEN=" .env; then
    echo "‚ùå Erreur: HF_TOKEN non configur√© dans .env"
    echo "üí° Ajoutez votre token HuggingFace dans le fichier .env"
    exit 1
fi

# Afficher la configuration
echo "‚úÖ Configuration d√©tect√©e:"
grep "HF_USERNAME=" .env || echo "‚ö†Ô∏è  HF_USERNAME non d√©fini"
grep "ARXIV_IMAGE_SAMPLES=" .env || echo "‚ÑπÔ∏è  ARXIV_IMAGE_SAMPLES: utilise d√©faut (50000)"

echo ""
echo "üìä Ce script va traiter ~1.29M √©chantillons"
echo "‚è±Ô∏è  Temps estim√©: 2-4 heures (6x plus rapide!)"
echo "üîÑ Traitement parall√®le: 7 workers (1 par dataset)"
echo "üì§ Upload automatique tous les 5000 samples"
echo "üíæ Espace √©conomis√©: 6x moins de stockage (images originales uniquement)"
echo ""
echo "üí° NOTES:"
echo "  ‚Ä¢ Vous pouvez interrompre (Ctrl+C) et reprendre plus tard"
echo "  ‚Ä¢ Les datasets seront visibles sur HuggingFace au fur et √† mesure"
echo "  ‚Ä¢ Les logs d√©taill√©s sont dans 'parallel_datasets.log'"
echo ""

# Nettoyer les anciens work directories de test
echo "üßπ Nettoyage des anciens work directories de test..."
if ls work_separate* work_* 2>/dev/null | grep -q .; then
    rm -rf work_separate* work_* 2>/dev/null || true
    echo "‚úÖ Anciens work directories supprim√©s"
else
    echo "‚úÖ Aucun ancien work directory √† nettoyer"
fi

echo ""
echo "üöÄ D√âMARRAGE DU TRAITEMENT..."
echo "============================================================="
echo ""

# Lancer le pipeline Python
PYTHONPATH=./src python3 run_full_pipeline.py

EXIT_CODE=$?

echo ""
echo "============================================================="
if [ $EXIT_CODE -eq 0 ]; then
    echo "‚úÖ TRAITEMENT TERMIN√â AVEC SUCC√àS!"
else
    echo "‚ö†Ô∏è  Le traitement s'est termin√© avec des erreurs (code: $EXIT_CODE)"
    echo "üí° Consultez 'parallel_datasets.log' pour les d√©tails"
fi
echo "============================================================="

exit $EXIT_CODE

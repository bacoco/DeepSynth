# ğŸŒ Guide de Workflow Cloud - Architecture SÃ©parÃ©e

## Vue d'Ensemble

Ce guide dÃ©crit l'architecture sÃ©parÃ©e de DeepSynth:
- **Phase 1**: GÃ©nÃ©ration de datasets (CPU, local/cloud) â†’ Upload HuggingFace
- **Phase 2**: Fine-tuning (GPU, cloud) â†’ Charge datasets depuis HuggingFace

### Avantages de cette Architecture

âœ… **SÃ©paration des Concerns**
- GÃ©nÃ©ration d'images â‰  Fine-tuning
- CPU pour preprocessing, GPU pour training
- Ã‰quipes diffÃ©rentes peuvent travailler en parallÃ¨le

âœ… **RÃ©utilisabilitÃ©**
- Datasets gÃ©nÃ©rÃ©s une seule fois
- RÃ©utilisables pour plusieurs expÃ©riences
- Partageables entre membres d'Ã©quipe

âœ… **ParallÃ©lisation**
- GÃ©nÃ©rer plusieurs datasets simultanÃ©ment
- Machines CPU bon marchÃ© pour preprocessing
- GPU coÃ»teux uniquement pour training

âœ… **EfficacitÃ© CoÃ»t**
- CPU instances: $0.05/h pour gÃ©nÃ©ration
- GPU instances: $1-3/h uniquement quand nÃ©cessaire
- Storage HF gratuit (public) ou peu coÃ»teux (privÃ©)

---

## ğŸ“‹ Architecture ComplÃ¨te

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              PHASE 1: DATASET GENERATION (CPU)              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Machine 1 (CPU):                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  MLSUM FR   â”‚  â†’   â”‚ Text-to-Imageâ”‚  â†’  â”‚ HuggingFace â”‚ â”‚
â”‚  â”‚   392k      â”‚      â”‚  Conversion  â”‚     â”‚   Dataset   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                             â”‚
â”‚  Machine 2 (CPU):                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  MLSUM ES   â”‚  â†’   â”‚ Text-to-Imageâ”‚  â†’  â”‚ HuggingFace â”‚ â”‚
â”‚  â”‚   266k      â”‚      â”‚  Conversion  â”‚     â”‚   Dataset   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                             â”‚
â”‚  Machine 3 (CPU):                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ CNN/DailyMailâ”‚  â†’  â”‚ Text-to-Imageâ”‚  â†’  â”‚ HuggingFace â”‚ â”‚
â”‚  â”‚   287k      â”‚      â”‚  Conversion  â”‚     â”‚   Dataset   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                              â¬‡ï¸

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           PHASE 2: FINE-TUNING (GPU, HuggingFace)          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  GPU Instance:                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ HuggingFace  â”‚  â†’  â”‚ Combine      â”‚  â†’  â”‚ Fine-Tune   â”‚ â”‚
â”‚  â”‚ Datasets     â”‚     â”‚ Datasets     â”‚     â”‚ DeepSeek    â”‚ â”‚
â”‚  â”‚ (3 datasets) â”‚     â”‚              â”‚     â”‚ OCR         â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                             â”‚
â”‚                                             â¬‡ï¸              â”‚
â”‚                                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚                                      â”‚ Trained     â”‚        â”‚
â”‚                                      â”‚ Model       â”‚        â”‚
â”‚                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Workflow Complet

### Ã‰tape 1: GÃ©nÃ©ration des Datasets (Phase Preprocessing)

#### 1.1 Configuration Initiale

```bash
# Configurer les credentials HuggingFace
cp .env.example .env
# Ã‰diter .env pour ajouter:
# HF_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxx
# HF_USERNAME=your-username
```

#### 1.2 GÃ©nÃ©rer UN Dataset (Test)

```bash
# GÃ©nÃ©rer MLSUM French avec limite (pour test)
python scripts/dataset_generation/generate_dataset_cloud.py \
    mlsum_fr \
    --max-samples 100 \
    --private  # ou sans --private pour public
```

**Sortie attendue**:
```
ğŸš€ GÃ©nÃ©ration du dataset: mlsum_fr
   Source: mlsum
   Target: deepsynth-mlsum-fr-images

ğŸ“Š Processing split: train
   Loading mlsum (fr) - train
   Limited to 100 samples
   Processing 100 samples...
   âœ… Processed: 98/100 (errors: 2)

ğŸ“Š Processing split: validation
   ...

â¬†ï¸  Uploading to HuggingFace: your-username/deepsynth-mlsum-fr-images

âœ… Dataset uploadÃ©: https://huggingface.co/datasets/your-username/deepsynth-mlsum-fr-images
```

#### 1.3 GÃ©nÃ©rer TOUS les Datasets (Production)

**Option A: SÃ©quentiellement sur une machine**
```bash
python scripts/dataset_generation/generate_dataset_cloud.py all
```

**Option B: En parallÃ¨le sur plusieurs machines**

Machine 1 (CPU):
```bash
python scripts/dataset_generation/generate_dataset_cloud.py mlsum_fr
python scripts/dataset_generation/generate_dataset_cloud.py mlsum_es
```

Machine 2 (CPU):
```bash
python scripts/dataset_generation/generate_dataset_cloud.py mlsum_de
python scripts/dataset_generation/generate_dataset_cloud.py cnn_dailymail
```

Machine 3 (CPU):
```bash
python scripts/dataset_generation/generate_dataset_cloud.py xsum
python scripts/dataset_generation/generate_dataset_cloud.py billsum
```

**Avantage**: 3x plus rapide car parallÃ¨le!

#### 1.4 VÃ©rifier les Datasets GÃ©nÃ©rÃ©s

```bash
python scripts/dataset_generation/generate_dataset_cloud.py mlsum_fr --list
```

**Sortie**:
```
ğŸ“‹ Datasets gÃ©nÃ©rÃ©s:
   âœ… your-username/deepsynth-mlsum-fr-images
   âœ… your-username/deepsynth-mlsum-es-images
   âœ… your-username/deepsynth-mlsum-de-images
   âœ… your-username/deepsynth-cnn-dailymail-images
   âœ… your-username/deepsynth-xsum-images
   âŒ your-username/deepsynth-billsum-images (pas encore gÃ©nÃ©rÃ©)
```

---

### Ã‰tape 2: Fine-Tuning depuis le Cloud (Phase Training)

#### 2.1 EntraÃ®ner avec UN Dataset

```bash
python scripts/training/train_from_cloud_datasets.py \
    --datasets mlsum_fr \
    --batch-size 8 \
    --epochs 3 \
    --mixed-precision bf16 \
    --output-dir ./checkpoints/mlsum-fr-only
```

#### 2.2 EntraÃ®ner avec PLUSIEURS Datasets (Multilingue)

```bash
python scripts/training/train_from_cloud_datasets.py \
    --datasets mlsum_fr mlsum_es mlsum_de \
    --batch-size 4 \
    --epochs 3 \
    --mixed-precision bf16 \
    --output-dir ./checkpoints/multilingual
```

#### 2.3 EntraÃ®ner avec TOUS les Datasets

```bash
python scripts/training/train_from_cloud_datasets.py \
    --datasets all \
    --batch-size 4 \
    --epochs 3 \
    --output-dir ./checkpoints/all-datasets
```

#### 2.4 Configuration AvancÃ©e

```bash
python scripts/training/train_from_cloud_datasets.py \
    --datasets mlsum_fr cnn_dailymail \
    --max-train-samples 50000 \
    --max-val-samples 5000 \
    --batch-size 16 \
    --epochs 5 \
    --learning-rate 1e-5 \
    --mixed-precision fp16 \
    --num-workers 8 \
    --gradient-accumulation 2 \
    --output-dir ./checkpoints/custom
```

---

## ğŸ“Š Cas d'Usage Pratiques

### Cas 1: Recherche Rapide (ItÃ©ration)

**Objectif**: Tester rapidement une idÃ©e

```bash
# GÃ©nÃ©rer petit dataset (une fois)
python scripts/dataset_generation/generate_dataset_cloud.py \
    mlsum_fr --max-samples 1000

# ItÃ©rer rapidement sur diffÃ©rentes configs
python scripts/training/train_from_cloud_datasets.py \
    --datasets mlsum_fr \
    --max-train-samples 800 \
    --max-val-samples 200 \
    --epochs 1 \
    --batch-size 16
```

### Cas 2: Production Multilingue

**Objectif**: ModÃ¨le final multilingue

```bash
# GÃ©nÃ©rer tous les datasets (une fois, peut prendre plusieurs jours)
# ParallÃ©liser sur 6 machines CPU
for dataset in mlsum_fr mlsum_es mlsum_de cnn_dailymail xsum billsum
do
    python scripts/dataset_generation/generate_dataset_cloud.py $dataset &
done

# Attendre que tous soient uploadÃ©s sur HuggingFace

# EntraÃ®ner le modÃ¨le final (GPU puissant)
python scripts/training/train_from_cloud_datasets.py \
    --datasets all \
    --batch-size 32 \
    --epochs 10 \
    --learning-rate 2e-5 \
    --mixed-precision bf16 \
    --output-dir ./checkpoints/production
```

### Cas 3: Fine-Tuning SpÃ©cialisÃ©

**Objectif**: ModÃ¨le spÃ©cialisÃ© pour le domaine lÃ©gal

```bash
# GÃ©nÃ©rer uniquement BillSum
python scripts/dataset_generation/generate_dataset_cloud.py billsum

# Fine-tuning spÃ©cialisÃ©
python scripts/training/train_from_cloud_datasets.py \
    --datasets billsum \
    --batch-size 8 \
    --epochs 5 \
    --output-dir ./checkpoints/legal-specialist
```

### Cas 4: Transfer Learning

**Objectif**: PrÃ©-entraÃ®ner sur donnÃ©es gÃ©nÃ©rales, puis spÃ©cialiser

```bash
# Phase 1: PrÃ©-entraÃ®nement sur donnÃ©es gÃ©nÃ©rales
python scripts/training/train_from_cloud_datasets.py \
    --datasets cnn_dailymail xsum \
    --epochs 3 \
    --output-dir ./checkpoints/general

# Phase 2: Fine-tuning sur domaine spÃ©cifique
python scripts/training/train_from_cloud_datasets.py \
    --datasets mlsum_fr \
    --model-name ./checkpoints/general/best_model \
    --epochs 2 \
    --learning-rate 1e-5 \
    --output-dir ./checkpoints/french-specialist
```

---

## ğŸ’° Estimation de CoÃ»ts

### GÃ©nÃ©ration de Datasets (Phase 1)

**Ressources**: CPU-only, ~4 cores, 16GB RAM

| Dataset | Samples | Temps EstimÃ© | CoÃ»t AWS (c5.xlarge $0.17/h) |
|---------|---------|--------------|------------------------------|
| MLSUM FR | 392k | 20h | $3.40 |
| MLSUM ES | 266k | 14h | $2.38 |
| MLSUM DE | 220k | 12h | $2.04 |
| CNN/DM | 287k | 15h | $2.55 |
| XSum | 204k | 11h | $1.87 |
| BillSum | 22k | 2h | $0.34 |
| **TOTAL** | **1.29M** | **74h** | **$12.58** |

**Avec parallÃ©lisation (6 machines)**:
- Temps: 20h (le plus long)
- CoÃ»t: 6 Ã— $0.17/h Ã— 20h = **$20.40**

### Fine-Tuning (Phase 2)

**Ressources**: GPU (A100 ou V100), 40GB+ VRAM

| Configuration | GPU | Temps | CoÃ»t AWS (p3.2xlarge $3.06/h) |
|---------------|-----|-------|-------------------------------|
| Test (1k samples) | V100 | 0.5h | $1.53 |
| Medium (50k) | V100 | 5h | $15.30 |
| Full (1.29M) | A100 | 48h | $146.88 |

### Comparaison: CouplÃ© vs SÃ©parÃ©

**Ancien (CouplÃ©)**:
- GPU pour TOUT: gÃ©nÃ©ration + training
- Temps GPU: 74h + 48h = 122h
- CoÃ»t: 122h Ã— $3.06/h = **$373.32**

**Nouveau (SÃ©parÃ©)**:
- CPU pour gÃ©nÃ©ration: 20h Ã— $0.17/h Ã— 6 = $20.40
- GPU pour training: 48h Ã— $3.06/h = $146.88
- **Total: $167.28** (Ã©conomie de **$206** soit 55%)

---

## ğŸ”§ Configuration AvancÃ©e

### Datasets PrivÃ©s vs Publics

**PrivÃ©** (par dÃ©faut recommandÃ©):
```bash
python scripts/dataset_generation/generate_dataset_cloud.py \
    mlsum_fr --private
```

**Public** (pour partager avec la communautÃ©):
```bash
python scripts/dataset_generation/generate_dataset_cloud.py \
    mlsum_fr  # sans --private
```

### Utiliser des Datasets d'un Autre Utilisateur

Si quelqu'un d'autre a dÃ©jÃ  gÃ©nÃ©rÃ© les datasets:

```python
# Modifier dans scripts/training/train_from_cloud_datasets.py
# Ligne ~30
AVAILABLE_DATASETS = {
    "mlsum_fr": "other-user/deepsynth-mlsum-fr-images",  # Changez le username
    ...
}
```

Ou crÃ©er votre propre mapping:
```bash
python scripts/training/train_from_cloud_datasets.py \
    --datasets mlsum_fr \
    --hf-dataset-prefix "other-user"
```

---

## ğŸ“ˆ Workflow CI/CD RecommandÃ©

### GitHub Actions - GÃ©nÃ©ration Automatique

```yaml
# .github/workflows/generate-datasets.yml
name: Generate Datasets

on:
  workflow_dispatch:
    inputs:
      dataset:
        description: 'Dataset to generate'
        required: true
        type: choice
        options:
          - mlsum_fr
          - mlsum_es
          - all

jobs:
  generate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
      - name: Install dependencies
        run: pip install -r requirements.txt
      - name: Generate dataset
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          HF_USERNAME: ${{ secrets.HF_USERNAME }}
        run: |
          python scripts/dataset_generation/generate_dataset_cloud.py \
            ${{ github.event.inputs.dataset }}
```

---

## âš¡ Conseils de Performance

### 1. ParallÃ©lisation Optimale

```bash
# Utiliser GNU Parallel pour lancer plusieurs gÃ©nÃ©rations
parallel -j 3 \
    python scripts/dataset_generation/generate_dataset_cloud.py {} \
    ::: mlsum_fr mlsum_es mlsum_de cnn_dailymail xsum billsum
```

### 2. Chunking pour TrÃ¨s Gros Datasets

Si un dataset est trop gros pour une seule machine:

```python
# Modifier le script pour gÃ©nÃ©rer par chunks
python scripts/dataset_generation/generate_dataset_cloud.py \
    mlsum_fr \
    --start-index 0 \
    --end-index 100000 \
    --chunk-id 1

python scripts/dataset_generation/generate_dataset_cloud.py \
    mlsum_fr \
    --start-index 100000 \
    --end-index 200000 \
    --chunk-id 2
```

### 3. Caching Local

Pour Ã©viter de tÃ©lÃ©charger Ã  chaque fois:

```bash
export HF_DATASETS_CACHE="/mnt/large-disk/hf-cache"
python scripts/training/train_from_cloud_datasets.py --datasets all
```

---

## ğŸ“ Bonnes Pratiques

### âœ… Ã€ FAIRE

1. **GÃ©nÃ©rer une fois, utiliser plusieurs fois**
   - GÃ©nÃ©rer les datasets et les uploader
   - RÃ©utiliser pour diffÃ©rentes expÃ©riences

2. **Versioning des datasets**
   - Ajouter dates dans les noms: `deepsynth-mlsum-fr-images-2025-10`
   - Permet de comparer diffÃ©rentes versions

3. **Documentation des datasets**
   - Les READMEs sont auto-gÃ©nÃ©rÃ©s
   - Ajouter dÃ©tails supplÃ©mentaires si nÃ©cessaire

4. **Tests avec petits Ã©chantillons**
   - Toujours tester avec `--max-samples 100` d'abord
   - VÃ©rifier que tout fonctionne avant gÃ©nÃ©ration complÃ¨te

### âŒ Ã€ Ã‰VITER

1. **RÃ©gÃ©nÃ©rer Ã  chaque training**
   - CoÃ»teux et inutile
   - Utiliser datasets cloud existants

2. **Datasets publics avec donnÃ©es sensibles**
   - Toujours utiliser `--private` pour donnÃ©es propriÃ©taires

3. **Oublier la limite de samples**
   - Peut saturer le quota HuggingFace
   - Commencer petit puis scaler

---

## ğŸ” Troubleshooting

### ProblÃ¨me: Dataset pas trouvÃ©

**Erreur**: `Dataset not found: your-username/deepsynth-mlsum-fr-images`

**Solution**:
```bash
# VÃ©rifier que le dataset existe
python scripts/dataset_generation/generate_dataset_cloud.py mlsum_fr --list

# VÃ©rifier les credentials
echo $HF_TOKEN
echo $HF_USERNAME
```

### ProblÃ¨me: Out of Memory durant gÃ©nÃ©ration

**Solution**:
```bash
# Utiliser une machine avec plus de RAM
# Ou rÃ©duire le batch de traitement
# Modifier dans generate_dataset_cloud.py:
# Traiter par plus petits batches
```

### ProblÃ¨me: Upload HuggingFace Ã©choue

**Solution**:
```bash
# VÃ©rifier la connexion
huggingface-cli whoami

# Re-login si nÃ©cessaire
huggingface-cli login
```

---

## ğŸ“ Support

- **Documentation**: Ce guide
- **Scripts**: `scripts/dataset_generation/` et `scripts/training/`
- **Issues**: GitHub Issues
- **HuggingFace**: https://huggingface.co/docs/datasets

---

**PrÃªt Ã  scaler! ğŸš€**
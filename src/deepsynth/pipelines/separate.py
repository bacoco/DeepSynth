#!/usr/bin/env python3
"""
Pipeline pour cr√©er des datasets s√©par√©s par langue/type sur HuggingFace.
Chaque dataset aura un nom de la forme: deepsynth-fr, deepsynth-es, etc.
"""

import os
import json
import pickle
from pathlib import Path
from datasets import Dataset, DatasetDict, load_dataset
from huggingface_hub import login, whoami, HfApi

from deepsynth.data.text_to_image import TextToImageConverter
from deepsynth.data.mlsum_loader import MLSUMLoader

# Load environment variables
env_file = Path('.env')
if env_file.exists():
    with open(env_file) as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith('#') and '=' in line:
                key, value = line.split('=', 1)
                os.environ[key] = value

# Mapping des datasets vers leurs noms de sortie
DATASET_NAMING = {
    ('MLSUM', 'fr'): 'deepsynth-fr',                    # Fran√ßais - 392k exemples
    ('MLSUM', 'es'): 'deepsynth-es',                    # Espagnol - 266k exemples
    ('MLSUM', 'de'): 'deepsynth-de',                    # Allemand - 220k exemples
    ('cnn_dailymail', '3.0.0'): 'deepsynth-en-news',   # Anglais actualit√©s - 287k exemples
    ('ccdv/arxiv-summarization', None): 'deepsynth-en-arxiv',  # Anglais scientifique - 50k exemples
    ('Rexhaif/xsum_reduced', None): 'deepsynth-en-xsum',       # Anglais BBC - 50k exemples
    ('billsum', None): 'deepsynth-en-legal',                   # Anglais juridique - 22k exemples
}

class OptimizedConverter(TextToImageConverter):
    def __init__(self):
        # Use DejaVu Sans font for proper French character support
        unicode_font_path = '/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf'
        super().__init__(
            font_path=unicode_font_path,
            font_size=12, max_width=1600, max_height=2400, margin=40,
            background_color=(255, 255, 255), text_color=(0, 0, 0)
        )

class SeparateDatasetBuilder:
    def __init__(self, work_dir="./work_separate"):
        self.work_dir = Path(work_dir)
        self.work_dir.mkdir(exist_ok=True)
        self.progress_file = self.work_dir / "progress_separate.json"
        self.converter = OptimizedConverter()
        self.progress = self.load_progress()

    def load_progress(self):
        if self.progress_file.exists():
            with open(self.progress_file, 'r') as f:
                return json.load(f)
        return {'completed_datasets': []}

    def save_progress(self):
        with open(self.progress_file, 'w') as f:
            json.dump(self.progress, f, indent=2)

    def _extract_text_and_summary(self, example, name, text_field, summary_field):
        """Extract text and summary from example based on dataset type"""
        try:
            if name == 'cnn_dailymail':
                return example.get('article', ''), example.get('highlights', '')
            elif name == 'billsum':
                return example.get('text', ''), example.get('summary', '')
            elif 'xsum' in name.lower():
                text = example.get('text', example.get('document', ''))
                summary = example.get('target', example.get('summary', ''))
                return text, summary
            else:
                # Standard processing using provided field names (works for MLSUM)
                return example.get(text_field, ''), example.get(summary_field, '')
        except Exception as e:
            print(f"      ‚ö† Error extracting fields: {e}")
            return '', ''

    def get_dataset_output_name(self, name, subset):
        """Get the output dataset name for HuggingFace"""
        key = (name, subset)
        if key in DATASET_NAMING:
            return DATASET_NAMING[key]
        else:
            # Fallback naming
            if subset:
                return f"deepsynth-{subset}"
            else:
                return f"deepsynth-{name.replace('/', '-')}"

    def check_existing_dataset_progress(self, repo_name):
        """Check existing dataset on HuggingFace and return progress info"""
        try:
            print(f"    üîç V√©rification du dataset existant: {repo_name}")

            # Try to load existing dataset
            existing_dataset = load_dataset(repo_name, split='train')

            if 'original_index' in existing_dataset.column_names:
                existing_indices = set(existing_dataset['original_index'])
                last_index = max(existing_indices) if existing_indices else -1
                total_processed = len(existing_indices)

                print(f"    üìä Dataset existant trouv√©:")
                print(f"      - {total_processed} √©chantillons d√©j√† trait√©s")
                print(f"      - Dernier index trait√©: {last_index}")

                return {
                    'exists': True,
                    'processed_indices': existing_indices,
                    'last_index': last_index,
                    'total_processed': total_processed,
                    'existing_dataset': existing_dataset
                }
            else:
                print(f"    ‚ö†Ô∏è Dataset existant sans colonne 'original_index', recommence depuis le d√©but")
                return {'exists': False}

        except Exception as e:
            print(f"    ‚ÑπÔ∏è Aucun dataset existant trouv√© (normal pour nouveau dataset)")
            return {'exists': False}

    def process_and_upload_dataset(self, name, subset, text_field, summary_field, username, *, max_samples=None):
        """Process a single dataset and upload it immediately to HuggingFace"""

        # Check if already completed
        dataset_key = f"{name}_{subset}" if subset else name
        if dataset_key in self.progress['completed_datasets']:
            print(f"‚úÖ {dataset_key} d√©j√† compl√©t√© et upload√©")
            return

        output_name = self.get_dataset_output_name(name, subset)
        repo_name = f"{username}/{output_name}"

        print(f"\\nüì• Traitement: {name} ({subset}) ‚Üí {repo_name}")

        # Check existing progress on HuggingFace
        progress_info = self.check_existing_dataset_progress(repo_name)

        # D√©terminer les splits
        splits_map = {
            'MLSUM': ['train', 'validation', 'test'],
            'billsum': ['train', 'test'],
            'cnn_dailymail': ['train', 'validation', 'test'],
            'ccdv/arxiv-summarization': ['train'],
            'Rexhaif/xsum_reduced': ['train'],
        }
        splits = splits_map.get(name, ['train'])

        # Start with existing samples if any
        if progress_info['exists']:
            all_samples = []
            existing_dataset = progress_info['existing_dataset']

            # Convert existing dataset back to list of samples
            for i in range(len(existing_dataset)):
                all_samples.append({
                    'text': existing_dataset[i]['text'],
                    'summary': existing_dataset[i]['summary'],
                    'image': existing_dataset[i]['image'],
                    'source_dataset': existing_dataset[i]['source_dataset'],
                    'original_split': existing_dataset[i]['original_split'],
                    'original_index': existing_dataset[i]['original_index'],
                })

            processed_indices = progress_info['processed_indices']
            last_index = progress_info['last_index']
            print(f"    üîÑ Reprise √† partir de l'index {last_index + 1}")
        else:
            all_samples = []
            processed_indices = set()
            last_index = -1

        for split in splits:
            print(f"  üìÇ Processing split: {split}")

            try:
                # Load dataset
                if name == 'MLSUM':
                    print(f"    üéØ Loading MLSUM {subset} using custom loader")
                    mlsum_loader = MLSUMLoader()
                    dataset_dict = mlsum_loader.load_language(subset)

                    if split in dataset_dict:
                        dataset = dataset_dict[split]
                        print(f"    ‚úÖ MLSUM {subset} {split} loaded successfully")
                    else:
                        print(f"    ‚ùå Split {split} not found for MLSUM {subset}")
                        continue
                else:
                    # Try loading with different strategies
                    dataset = None
                    try:
                        if subset:
                            dataset = load_dataset(name, subset, split=split)
                        else:
                            dataset = load_dataset(name, split=split)
                        print(f"    ‚úÖ Direct loading successful")
                    except Exception as e:
                        try:
                            if subset:
                                dataset = load_dataset(name, subset, split=split, trust_remote_code=True)
                            else:
                                dataset = load_dataset(name, split=split, trust_remote_code=True)
                            print(f"    ‚úÖ Trust remote code successful")
                        except Exception as e2:
                            print(f"    ‚ùå Failed to load {name}: {e2}")
                            continue

                total = len(dataset)
                limit = min(total, max_samples) if max_samples is not None else total

                # Calculate remaining work
                remaining_indices = []
                for idx in range(limit):
                    # Create a unique key for this sample across all splits
                    sample_key = f"{split}_{idx}"
                    if idx not in processed_indices:
                        remaining_indices.append(idx)

                if not remaining_indices:
                    print(f"    ‚úÖ {split} d√©j√† compl√®tement trait√© ({limit} √©chantillons)")
                    continue

                print(f"    üìä {len(remaining_indices)} √©chantillons restants √† traiter sur {limit}")
                print(f"    üìà Progression: {((limit - len(remaining_indices)) / limit * 100):.1f}% d√©j√† fait")

                # Process only remaining samples
                split_samples = []
                processed_count = 0

                for idx in remaining_indices:
                    processed_count += 1
                    if processed_count % 1000 == 0:
                        print(f"      üìà {processed_count}/{len(remaining_indices)} ({processed_count/len(remaining_indices)*100:.1f}%)")

                    example = dataset[idx]
                    text, summary = self._extract_text_and_summary(example, name, text_field, summary_field)

                    if not text or not summary:
                        continue

                    try:
                        image = self.converter.convert(text)
                        split_samples.append({
                            'text': text,
                            'summary': summary,
                            'image': image,
                            'source_dataset': name,
                            'original_split': split,
                            'original_index': idx
                        })
                    except Exception as e:
                        print(f"      ‚ùå Sample {idx}: {e}")
                        continue

                all_samples.extend(split_samples)
                print(f"    ‚úÖ {split} completed: {len(split_samples)} nouveaux √©chantillons trait√©s")

            except Exception as e:
                print(f"    ‚ùå Error processing {split}: {e}")
                continue

        if not all_samples:
            print(f"    ‚ùå No samples processed for {name}")
            return

        print(f"\\nüîß Creating dataset with {len(all_samples)} samples...")

        # Create dataset
        dataset = Dataset.from_dict({
            'text': [s['text'] for s in all_samples],
            'summary': [s['summary'] for s in all_samples],
            'image': [s['image'] for s in all_samples],
            'source_dataset': [s['source_dataset'] for s in all_samples],
            'original_split': [s['original_split'] for s in all_samples],
            'original_index': [s['original_index'] for s in all_samples],
        })

        from datasets import Image as ImageFeature
        dataset = dataset.cast_column("image", ImageFeature())

        # Pas de split train/validation - tout reste dans 'train'
        # Le split se fera au moment de l'entra√Ænement selon les besoins
        dataset_dict = DatasetDict({'train': dataset})

        # Upload to HuggingFace
        try:
            if progress_info['exists']:
                print(f"\\nüì§ Mise √† jour du dataset existant: {repo_name}")
                print(f"    üìä Total final: {len(all_samples)} √©chantillons")
            else:
                print(f"\\nüì§ Cr√©ation du nouveau dataset: {repo_name}")
                print(f"    üìä Total: {len(all_samples)} √©chantillons")

            dataset_dict.push_to_hub(repo_name, private=False, token=os.getenv('HF_TOKEN'))

            print(f"‚úÖ Successfully uploaded: https://huggingface.co/datasets/{repo_name}")

            # Mark as completed
            self.progress['completed_datasets'].append(dataset_key)
            self.save_progress()

        except Exception as e:
            print(f"‚ùå Failed to upload {repo_name}: {e}")
            raise

def main():
    print("üéØ CR√âATION DE DATASETS S√âPAR√âS PAR LANGUE")
    print("=" * 60)

    login(token=os.getenv('HF_TOKEN'))
    username = whoami()['name']

    print(f"Username: {username}")
    print("\\nüìä Datasets √† cr√©er:")
    for (name, subset), output_name in DATASET_NAMING.items():
        print(f"  - {name} ({subset}) ‚Üí {username}/{output_name}")

    builder = SeparateDatasetBuilder()

    # Get arXiv limit
    try:
        arxiv_limit = int(os.getenv('ARXIV_IMAGE_SAMPLES', '50000'))
    except ValueError:
        print("‚ö†Ô∏è  ARXIV_IMAGE_SAMPLES invalide. Utilisation de 50000 par d√©faut.")
        arxiv_limit = 50000
    if arxiv_limit <= 0:
        print("‚ö†Ô∏è  ARXIV_IMAGE_SAMPLES doit √™tre positif. Utilisation de 10000 par d√©faut.")
        arxiv_limit = 10000

    # Define sources with their configurations - ORDRE DE PRIORIT√â
    sources = [
        # PRIORIT√â 1: CNN/DailyMail (Anglais actualit√©s) - Le plus demand√©
        ('cnn_dailymail', '3.0.0', 'article', 'highlights', None),

        # PRIORIT√â 2: arXiv (Anglais scientifique) - Recherche
        ('ccdv/arxiv-summarization', None, 'article', 'abstract', arxiv_limit),

        # PRIORIT√â 3: XSum BBC (Anglais BBC News) - Actualit√©s courtes
        ('Rexhaif/xsum_reduced', None, 'text', 'target', None),

        # PRIORIT√â 4: Fran√ßais MLSUM - Multilingue commence ici
        ('MLSUM', 'fr', 'text', 'summary', None),

        # PRIORIT√â 5: Autres langues MLSUM
        ('MLSUM', 'es', 'text', 'summary', None),
        ('MLSUM', 'de', 'text', 'summary', None),

        # PRIORIT√â 6: Juridique (plus sp√©cialis√©)
        ('billsum', None, 'text', 'summary', None),
    ]

    try:
        for name, subset, text_field, summary_field, max_samples in sources:
            builder.process_and_upload_dataset(
                name, subset, text_field, summary_field, username,
                max_samples=max_samples
            )

        print(f"\\nüéâ TOUS LES DATASETS CR√â√âS AVEC SUCC√àS!")
        print("\\nüìä Datasets disponibles:")
        for (name, subset), output_name in DATASET_NAMING.items():
            print(f"  üîó https://huggingface.co/datasets/{username}/{output_name}")

        # Cleanup
        import shutil
        if builder.work_dir.exists():
            shutil.rmtree(builder.work_dir)
            print("\\nüßπ Nettoyage termin√©")

    except KeyboardInterrupt:
        print("\\n‚è∏Ô∏è Interruption - relancez pour continuer")
        builder.save_progress()
    except Exception as e:
        print(f"\\n‚ùå Erreur: {e}")
        builder.save_progress()
        raise

if __name__ == "__main__":
    main()

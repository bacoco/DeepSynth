#!/usr/bin/env python3
"""
Script de lancement pour le traitement parall√®le des datasets
"""

import os
import sys

from deepsynth.data.transforms.text_to_image import DEEPSEEK_OCR_RESOLUTIONS

from .parallel_datasets_builder import ParallelDatasetsPipeline

def run_parallel_datasets_cli():
    """Point d'entr√©e principal"""
    print("üöÄ TRAITEMENT PARALL√àLE DES DATASETS DEEPSYNTH")
    print("="*60)
    print("Ce script traite plusieurs datasets en parall√®le pour acc√©l√©rer le processus.")
    print("Chaque dataset est ind√©pendant et sera upload√© sur HuggingFace s√©par√©ment.")
    print()
    
    # V√©rifier le token HuggingFace
    if not os.getenv('HF_TOKEN'):
        print("‚ùå Variable d'environnement HF_TOKEN manquante!")
        print("   Ajoutez votre token HuggingFace dans le fichier .env")
        return 1
    
    # Configuration
    print("‚öôÔ∏è CONFIGURATION")
    print("-" * 30)
    
    max_workers = input("Nombre de processus parall√®les (d√©faut: 3, max recommand√©: 4): ").strip()
    try:
        max_workers = int(max_workers) if max_workers else 3
        if max_workers < 1:
            max_workers = 1
        elif max_workers > 6:
            print("‚ö†Ô∏è Attention: Plus de 6 processus peut surcharger le syst√®me")
            max_workers = 6
    except ValueError:
        max_workers = 3
    
    print(f"‚úÖ Utilisation de {max_workers} processus parall√®les")

    # Multi-resolution: Always enabled with all sizes
    print("\nüîç MULTI-R√âSOLUTION (DeepSeek OCR)")
    print("-" * 30)
    print("‚úÖ Multi-r√©solution activ√©e: TOUTES les r√©solutions seront g√©n√©r√©es")
    sizes_list = list(DEEPSEEK_OCR_RESOLUTIONS.items())
    for idx, (name, size) in enumerate(sizes_list, start=1):
        print(f"  {idx}. {name:<6} ({size[0]}√ó{size[1]})")

    multi_resolution = True
    resolution_sizes = None  # All sizes

    # Options de traitement
    print("\nüìã OPTIONS DE TRAITEMENT")
    print("-" * 30)
    print("1. Traiter tous les datasets (recommand√©)")
    print("2. Traiter des datasets sp√©cifiques")
    print("3. Mode test rapide (500 √©chantillons par dataset)")

    choice = input("\nVotre choix (1-3): ").strip()

    # Cr√©er le pipeline
    pipeline = ParallelDatasetsPipeline(
        max_workers=max_workers,
        multi_resolution=multi_resolution,
        resolution_sizes=resolution_sizes
    )
    
    print(f"\nüìä DATASETS DISPONIBLES ({len(pipeline.datasets_config)} au total)")
    print("-" * 50)
    for i, dataset in enumerate(pipeline.datasets_config, 1):
        max_samples = dataset.get('max_samples')
        samples_info = f" (max {max_samples:,})" if max_samples else " (complet)"
        print(f"{i:2d}. {dataset['name']:<20} ‚Üí {dataset['output_name']}{samples_info}")
    
    try:
        if choice == "1":
            # Tous les datasets
            print(f"\nüöÄ D√âMARRAGE DU TRAITEMENT COMPLET")
            print("=" * 50)
            print("‚ö†Ô∏è  Ceci peut prendre plusieurs heures selon la taille des datasets")
            
            confirm = input("Continuer? (o/N): ").strip().lower()
            if confirm not in ['o', 'oui', 'y', 'yes']:
                print("‚ùå Annul√© par l'utilisateur")
                return 0
            
            results = pipeline.run_parallel_processing()
            
        elif choice == "2":
            # Datasets sp√©cifiques
            print(f"\nüìù S√âLECTION DE DATASETS")
            print("-" * 30)
            selected_indices = input("Indices des datasets √† traiter (ex: 1,2,3): ").strip()
            
            try:
                indices = [int(x.strip()) - 1 for x in selected_indices.split(",")]
                selected_datasets = []
                
                for i in indices:
                    if 0 <= i < len(pipeline.datasets_config):
                        selected_datasets.append(pipeline.datasets_config[i]['name'])
                    else:
                        print(f"‚ö†Ô∏è Index {i+1} invalide, ignor√©")
                
                if not selected_datasets:
                    print("‚ùå Aucun dataset valide s√©lectionn√©")
                    return 1
                
                print(f"\n‚úÖ Datasets s√©lectionn√©s: {', '.join(selected_datasets)}")
                results = pipeline.run_parallel_processing(selected_datasets=selected_datasets)
                
            except ValueError:
                print("‚ùå Format invalide")
                return 1
                
        elif choice == "3":
            # Mode test
            print(f"\nüß™ MODE TEST RAPIDE")
            print("-" * 30)
            print("Traitement de 500 √©chantillons maximum par dataset pour test")
            
            # Limiter tous les datasets √† 500 √©chantillons
            for dataset_config in pipeline.datasets_config:
                dataset_config['max_samples'] = 500
            
            # Demander quels datasets tester
            test_choice = input("Tester tous les datasets (o) ou seulement quelques-uns (n)? (o/N): ").strip().lower()
            
            if test_choice in ['o', 'oui', 'y', 'yes']:
                results = pipeline.run_parallel_processing()
            else:
                selected_indices = input("Indices des datasets √† tester (ex: 1,3): ").strip()
                try:
                    indices = [int(x.strip()) - 1 for x in selected_indices.split(",")]
                    selected_datasets = [
                        pipeline.datasets_config[i]['name']
                        for i in indices
                        if 0 <= i < len(pipeline.datasets_config)
                    ]
                    
                    if selected_datasets:
                        results = pipeline.run_parallel_processing(selected_datasets=selected_datasets)
                    else:
                        print("‚ùå Aucun dataset valide s√©lectionn√©")
                        return 1
                except ValueError:
                    print("‚ùå Format invalide")
                    return 1
        else:
            print("‚ùå Choix invalide")
            return 1
        
        # R√©sum√© final
        successful = [r for r in results if r['status'] == 'success']
        failed = [r for r in results if r['status'] == 'error']
        
        print(f"\nüéØ R√âSUM√â FINAL")
        print("=" * 40)
        print(f"‚úÖ R√©ussis: {len(successful)}")
        print(f"‚ùå √âchou√©s: {len(failed)}")
        
        if successful:
            print(f"\nüîó DATASETS CR√â√âS:")
            for result in successful:
                if 'repo_name' in result:
                    print(f"  - https://huggingface.co/datasets/{result['repo_name']}")
        
        if failed:
            print(f"\n‚ùå √âCHECS √Ä INVESTIGUER:")
            for result in failed:
                print(f"  - {result['dataset']}: {result.get('error', 'Erreur inconnue')}")
        
        print(f"\nüìã Consultez 'parallel_datasets.log' pour les d√©tails complets.")
        
        return 0 if len(failed) == 0 else 1
        
    except KeyboardInterrupt:
        print(f"\n‚è∏Ô∏è INTERRUPTION D√âTECT√âE")
        print("Le traitement peut √™tre relanc√© - il reprendra o√π il s'est arr√™t√© gr√¢ce √† la logique incr√©mentale.")
        return 0
    except Exception as e:
        print(f"\n‚ùå ERREUR CRITIQUE: {str(e)}")
        return 1

if __name__ == "__main__":
    exit_code = run_parallel_datasets_cli()
    sys.exit(exit_code)
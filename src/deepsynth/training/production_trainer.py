"""
Unified Production Trainer for DeepSynth with Vision-to-Text Training.

This trainer properly implements the vision→decoder training flow:
    Images → Frozen Vision Encoder (380M) → Visual Tokens (20x compression)
          → Trainable MoE Decoder (570M active) → Summary
"""

from __future__ import annotations

import json
import logging
import math
import time
from dataclasses import asdict
from pathlib import Path
from typing import Any, Dict, Iterable, Optional, Tuple, Union

import torch
import torch.nn as nn
from accelerate import Accelerator
from datasets import DatasetDict, load_dataset
from huggingface_hub import HfApi, create_repo
from PIL import Image
from torch.cuda.amp import GradScaler
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm
from transformers import AutoModel, AutoTokenizer, get_scheduler

from .config import TrainerConfig
from .model_utils import (
    freeze_vision_encoder,
    print_parameter_summary,
    validate_vision_model,
)
from .moe_dropout import ExpertGradientDropout, GateGradientDropout
from .text_encoder import TextEncoderModule
from deepsynth.data.transforms import create_training_transform, create_inference_transform

LOGGER = logging.getLogger(__name__)


class DeepSynthDataset(Dataset):
    """Dataset wrapper for DeepSynth training with image augmentation."""

    def __init__(
        self,
        data: Union[Iterable[dict], list],
        transform=None,
    ):
        """
        Initialize dataset.

        Args:
            data: Iterable of samples with 'image', 'summary', 'text' fields
            transform: Transform pipeline for images (e.g., create_training_transform())
        """
        # Convert to list if needed for indexing
        if isinstance(data, (list, tuple)):
            self.data = list(data)
        elif hasattr(data, '__iter__'):
            self.data = list(data)
        else:
            raise TypeError("Data must be iterable")

        self.transform = transform

    def __len__(self) -> int:
        return len(self.data)

    def __getitem__(self, idx: int) -> dict:
        """Get sample with optional image transformation."""
        sample = self.data[idx]

        # Ensure we have required fields
        if 'summary' not in sample:
            raise KeyError(f"Sample {idx} missing 'summary' field")

        # Handle image loading and transformation
        if 'image' in sample:
            image = sample['image']
            if isinstance(image, str):
                # Load from path
                image = Image.open(image).convert('RGB')
            elif not isinstance(image, Image.Image):
                # Assume it's already a PIL Image
                image = image.convert('RGB') if hasattr(image, 'convert') else image

            # Apply augmentation transform if provided
            if self.transform is not None:
                image = self.transform(image)

            sample = {**sample, 'image': image}

        return sample


class UnifiedProductionTrainer:
    """
    Production trainer for DeepSeek-OCR vision-to-text summarization.

    Properly implements:
    - Vision encoder freezing with validation
    - Image augmentation pipeline
    - Mixed precision training
    - Gradient accumulation
    - Distributed training support via accelerate
    - Checkpoint saving and resumption
    - HuggingFace Hub integration
    """

    def __init__(self, config: TrainerConfig):
        if not isinstance(config, TrainerConfig):
            raise TypeError("config must be an instance of TrainerConfig")

        self.config = config
        self.output_dir = Path(config.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # Initialize accelerator for distributed training and mixed precision
        self.accelerator = Accelerator(
            mixed_precision=config.mixed_precision,
            gradient_accumulation_steps=config.gradient_accumulation_steps,
        )

        LOGGER.info("Using device: %s", self.accelerator.device)
        LOGGER.info("Mixed precision: %s", config.mixed_precision)
        LOGGER.info("Gradient accumulation steps: %d", config.gradient_accumulation_steps)

        # Determine model source
        model_source = config.resume_from_checkpoint or config.model_name
        if config.resume_from_checkpoint:
            checkpoint_path = Path(config.resume_from_checkpoint)
            if not checkpoint_path.exists():
                raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")
            LOGGER.info("Resuming from checkpoint: %s", config.resume_from_checkpoint)

        # Load model with proper dtype
        dtype = torch.bfloat16 if config.mixed_precision == "bf16" else (
            torch.float16 if config.mixed_precision == "fp16" else torch.float32
        )

        LOGGER.info("Loading model: %s (dtype: %s)", model_source, dtype)
        self.model = AutoModel.from_pretrained(
            model_source,
            trust_remote_code=True,
            torch_dtype=dtype,
            device_map={"": self.accelerator.device},  # Proper device mapping for accelerate
        )

        self.tokenizer = AutoTokenizer.from_pretrained(
            model_source,
            trust_remote_code=True,
        )

        # Validate vision model architecture
        if not validate_vision_model(self.model):
            LOGGER.warning("⚠️ Model may not support vision-to-text training")

        # Freeze vision encoder using robust freezing utilities
        freeze_stats = freeze_vision_encoder(self.model)
        if freeze_stats['frozen_params'] == 0:
            raise RuntimeError("Failed to freeze vision encoder! No parameters frozen.")

        # Apply LoRA if enabled (before printing parameter summary)
        if config.use_lora or config.use_qlora:
            LOGGER.info("=" * 80)
            LOGGER.info("Applying LoRA/QLoRA Configuration")
            LOGGER.info("=" * 80)
            self._apply_lora()

        # Print detailed parameter summary
        print_parameter_summary(self.model)

        # Initialize text encoder if enabled (for instruction prompting)
        self.text_encoder = None
        if config.use_text_encoder:
            if not config.text_encoder_model:
                raise ValueError("use_text_encoder=True but text_encoder_model is not specified")

            LOGGER.info("=" * 80)
            LOGGER.info("Initializing Text Encoder for Instruction Prompting")
            LOGGER.info("=" * 80)

            self.text_encoder = TextEncoderModule(
                model_name=config.text_encoder_model,
                trainable=config.text_encoder_trainable,
                dtype=dtype,
                device=self.accelerator.device,
            )

            LOGGER.info("✅ Text encoder initialized successfully")
            LOGGER.info("  Model: %s", config.text_encoder_model)
            LOGGER.info("  Trainable: %s", config.text_encoder_trainable)
            LOGGER.info("  Output dim: 4096 (matches vision encoder)")

        # Setup dropout strategies
        self._setup_dropout_strategies()

        # Setup optimizer (include text encoder params if trainable)
        trainable_params = [p for p in self.model.parameters() if p.requires_grad]
        if self.text_encoder is not None and config.text_encoder_trainable:
            text_encoder_params = [p for p in self.text_encoder.parameters() if p.requires_grad]
            trainable_params.extend(text_encoder_params)
            LOGGER.info("Added %d text encoder parameters to optimizer", len(text_encoder_params))

        if not trainable_params:
            raise RuntimeError("No trainable parameters found!")

        self.optimizer = AdamW(
            trainable_params,
            lr=config.optimizer.learning_rate,
            weight_decay=config.optimizer.weight_decay,
        )

        # Setup gradient scaler for fp16
        self.scaler = GradScaler() if config.mixed_precision == "fp16" else None

        # HuggingFace Hub API
        self.api = HfApi()

        # Training state
        self.global_step = 0
        self.train_losses = []

    def _apply_lora(self) -> None:
        """
        Apply LoRA (Low-Rank Adaptation) or QLoRA to the model.

        This reduces trainable parameters from 2.9B to ~2-16M by using
        low-rank matrices instead of full fine-tuning.
        """
        from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
        from transformers import BitsAndBytesConfig

        config = self.config

        # For QLoRA: need to reload model with quantization
        if config.use_qlora:
            LOGGER.info("Applying QLoRA with %d-bit quantization", config.qlora_bits)

            # Create quantization config
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=(config.qlora_bits == 4),
                load_in_8bit=(config.qlora_bits == 8),
                bnb_4bit_quant_type=config.qlora_type,
                bnb_4bit_compute_dtype=torch.bfloat16 if config.mixed_precision == "bf16" else torch.float16,
                bnb_4bit_use_double_quant=config.qlora_double_quant,
            )

            # Reload model with quantization
            model_source = config.resume_from_checkpoint or config.model_name
            from transformers import AutoModel
            self.model = AutoModel.from_pretrained(
                model_source,
                trust_remote_code=True,
                quantization_config=bnb_config,
                device_map={"": self.accelerator.device},
            )

            # Prepare model for k-bit training
            self.model = prepare_model_for_kbit_training(self.model)

            LOGGER.info("✅ Model quantized to %d-bit", config.qlora_bits)

        # Auto-detect target modules if not specified
        target_modules = config.lora_target_modules
        if target_modules is None:
            # DeepSeek-OCR uses standard attention modules
            # Target q_proj, k_proj, v_proj, o_proj in decoder
            target_modules = [
                "q_proj",
                "k_proj",
                "v_proj",
                "o_proj",
            ]
            LOGGER.info("Auto-detected LoRA target modules: %s", target_modules)

        # Create LoRA configuration
        lora_config = LoraConfig(
            r=config.lora_rank,
            lora_alpha=config.lora_alpha,
            target_modules=target_modules,
            lora_dropout=config.lora_dropout,
            bias=config.lora_bias,
            task_type="CAUSAL_LM",  # For text generation
            modules_to_save=config.lora_modules_to_save,
        )

        # Apply LoRA to model
        self.model = get_peft_model(self.model, lora_config)

        # Log LoRA configuration
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in self.model.parameters())

        LOGGER.info("✅ LoRA applied successfully")
        LOGGER.info("  Rank (r): %d", config.lora_rank)
        LOGGER.info("  Alpha: %d", config.lora_alpha)
        LOGGER.info("  Dropout: %.3f", config.lora_dropout)
        LOGGER.info("  Target modules: %s", target_modules)
        LOGGER.info("  Trainable params: {:,} ({:.2%})".format(
            trainable_params, trainable_params / total_params
        ))
        LOGGER.info("  Total params: {:,}".format(total_params))

    def _setup_dropout_strategies(self) -> None:
        """Initialize optional dropout-based regularization strategies."""
        self.bidrop_passes = max(1, int(self.config.bidrop_passes))

        self._expert_dropout = (
            ExpertGradientDropout(
                self.model,
                dropout_rate=self.config.expert_dropout_rate,
                min_keep=self.config.expert_dropout_min_keep,
            )
            if self.config.expert_dropout_rate > 0
            else None
        )

        self._gate_dropout = (
            GateGradientDropout(
                self.model,
                dropout_rate=self.config.gate_dropout_rate,
                keywords=self.config.gate_dropout_keywords,
            )
            if self.config.gate_dropout_rate > 0
            else None
        )

        if self.bidrop_passes > 1:
            LOGGER.info("Bi-Drop enabled with %d subnet passes", self.bidrop_passes)

    def _create_dataloader(self, dataset: Dataset, shuffle: bool = True) -> DataLoader:
        """Create DataLoader with proper settings."""
        return DataLoader(
            dataset,
            batch_size=self.config.batch_size,
            shuffle=shuffle,
            num_workers=4,  # Parallel data loading
            pin_memory=True,  # Faster GPU transfer
            drop_last=True,  # Drop incomplete batches
            collate_fn=self._collate_batch,
        )

    def _preprocess_images_for_deepseek(self, pil_images: list) -> tuple:
        """
        Preprocess PIL images to DeepSeek-OCR format.
        Replicates the preprocessing logic from the infer() method.

        For images ≤640x640 (like our dataset):
        - Creates global view: pad to 1024x1024
        - No local crops needed
        - Returns tuple format: (crops, originals, spatial_crop)

        Args:
            pil_images: List of PIL images

        Returns:
            Tuple of (images_crop, images_ori, images_spatial_crop)
        """
        from PIL import ImageOps

        base_size = 1024  # Global view size
        images_ori_list = []
        images_spatial_crop_list = []

        # Image transform: normalize with mean=0.5, std=0.5
        from torchvision import transforms
        image_transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
        ])

        for img in pil_images:
            # Handle both PIL images and tensors
            if torch.is_tensor(img):
                # Image is already a tensor from dataset transform - skip padding
                # Just ensure it's the right dtype and size
                if img.shape[0] == 3:  # CHW format
                    global_tensor = img.to(torch.bfloat16)
                else:
                    global_tensor = img.permute(2, 0, 1).to(torch.bfloat16)  # HWC -> CHW
            else:
                # PIL image - apply padding and normalization
                global_view = ImageOps.pad(
                    img,
                    (base_size, base_size),
                    color=(127, 127, 127)  # Mean of 0.5 * 255
                )
                # Transform and convert to bfloat16
                global_tensor = image_transform(global_view).to(torch.bfloat16)

            images_ori_list.append(global_tensor)

            # No cropping for small images: [1, 1]
            images_spatial_crop_list.append([1, 1])

        # Stack into batched tensors
        images_ori = torch.stack(images_ori_list, dim=0)
        images_spatial_crop = torch.tensor(images_spatial_crop_list, dtype=torch.long)

        # No local crops for small images: return zeros placeholder
        batch_size = len(pil_images)
        images_crop = torch.zeros((batch_size, 3, base_size, base_size), dtype=torch.bfloat16)

        return images_crop, images_ori, images_spatial_crop

    def _collate_batch(self, batch: list[dict]) -> dict:
        """
        Collate batch with proper image and text processing.
        Creates prompts with <image> token and images_seq_mask like infer() does.

        Args:
            batch: List of samples with 'image', 'summary', and optional 'instruction' fields

        Returns:
            Dictionary with images, input_ids, images_seq_mask, labels
        """
        images = []
        summaries = []
        instructions = []

        for sample in batch:
            if 'image' not in sample or 'summary' not in sample:
                continue

            images.append(sample['image'])
            summaries.append(sample['summary'])

            # Collect instruction if present (for instruction prompting mode)
            if 'instruction' in sample and sample['instruction']:
                instructions.append(sample['instruction'])
            else:
                # Use default instruction if not provided
                instructions.append(self.config.instruction_prompt)

        if not summaries:
            raise ValueError("Batch contains no valid samples")

        # Preprocess images to DeepSeek-OCR format
        images_crop, images_ori, images_spatial_crop = self._preprocess_images_for_deepseek(images)

        # Create prompts with <image> token + summary (following infer() pattern)
        # For training: "<image>\nSummary: {summary}"
        batch_size = len(summaries)
        input_ids_list = []
        images_seq_mask_list = []
        labels_list = []

        # Image token constants (from infer() method)
        image_token_id = 128815
        patch_size = 16
        downsample_ratio = 4
        base_size = 1024
        num_queries_base = 16  # ceil((1024 // 16) / 4) = 16

        for i in range(batch_size):
            # Create prompt: "<image>\n" + summary
            # Tokenize prompt part
            prompt_text = "<image>\n"
            prompt_tokens = self.tokenizer.encode(prompt_text, add_special_tokens=True)

            # Tokenize summary (target)
            summary_tokens = self.tokenizer.encode(summaries[i], add_special_tokens=False)

            # Create image token sequence (273 tokens for 1024x1024 with no crops)
            # Formula from infer(): ([token_id] * num_queries_base + [token_id]) * num_queries_base + [token_id]
            num_image_tokens = (num_queries_base + 1) * num_queries_base + 1  # 273
            image_token_sequence = [image_token_id] * num_image_tokens

            # Build full sequence: prompt_tokens (before <image>) + image_tokens + summary_tokens
            # The <image> text token gets replaced by the 273 image token IDs
            # Find and replace <image> token
            image_text_token = self.tokenizer.encode("<image>", add_special_tokens=False)[0]
            prompt_tokens_replaced = []
            for tok in prompt_tokens:
                if tok == image_text_token:
                    prompt_tokens_replaced.extend(image_token_sequence)
                else:
                    prompt_tokens_replaced.append(tok)

            # Combine: prompt (with image tokens) + summary
            full_sequence = prompt_tokens_replaced + summary_tokens

            # Create images_seq_mask: False for text, True for image tokens
            mask = []
            for tok in prompt_tokens:
                if tok == image_text_token:
                    mask.extend([True] * num_image_tokens)  # Image tokens
                else:
                    mask.append(False)  # Text token
            mask.extend([False] * len(summary_tokens))  # Summary is text

            # Create labels: -100 for prompt, actual tokens for summary
            label_sequence = [-100] * len(prompt_tokens_replaced) + summary_tokens

            input_ids_list.append(full_sequence)
            images_seq_mask_list.append(mask)
            labels_list.append(label_sequence)

        # Pad sequences to max length
        max_len = max(len(seq) for seq in input_ids_list)
        max_len = min(max_len, self.config.max_length)

        input_ids_padded = []
        masks_padded = []
        labels_padded = []
        attention_masks = []

        for i in range(batch_size):
            seq = input_ids_list[i][:max_len]
            mask = images_seq_mask_list[i][:max_len]
            lab = labels_list[i][:max_len]

            pad_len = max_len - len(seq)
            input_ids_padded.append(seq + [self.tokenizer.pad_token_id] * pad_len)
            masks_padded.append(mask + [False] * pad_len)
            labels_padded.append(lab + [-100] * pad_len)
            attention_masks.append([1] * len(seq) + [0] * pad_len)

        return {
            "images_crop": images_crop,
            "images_ori": images_ori,
            "images_spatial_crop": images_spatial_crop,
            "input_ids": torch.tensor(input_ids_padded, dtype=torch.long),
            "attention_mask": torch.tensor(attention_masks, dtype=torch.long),
            "images_seq_mask": torch.tensor(masks_padded, dtype=torch.bool),
            "labels": torch.tensor(labels_padded, dtype=torch.long),
            "instructions": instructions if instructions else None,
        }

    def _forward_step(
        self,
        batch: dict,
    ) -> float:
        """
        Forward pass with proper vision-to-text flow.

        Optional instruction prompting:
        - If text_encoder enabled: encodes instructions and passes to model
        - Otherwise: standard vision-only forward pass

        Args:
            batch: Dictionary with 'images_crop', 'images_ori', 'images_spatial_crop',
                   'input_ids', 'attention_mask', 'labels', 'instructions'

        Returns:
            Loss value (float)
        """
        images_crop = batch["images_crop"]
        images_ori = batch["images_ori"]
        images_spatial_crop = batch["images_spatial_crop"]
        images_seq_mask = batch["images_seq_mask"]
        input_ids = batch["input_ids"]
        attention_mask = batch["attention_mask"]
        labels = batch["labels"]
        instructions = batch.get("instructions", None)

        # Move tensors to device
        images_crop = images_crop.to(self.accelerator.device)
        images_ori = images_ori.to(self.accelerator.device)
        images_spatial_crop = images_spatial_crop.to(self.accelerator.device)
        images_seq_mask = images_seq_mask.to(self.accelerator.device)
        input_ids = input_ids.to(self.accelerator.device)
        attention_mask = attention_mask.to(self.accelerator.device)
        labels = labels.to(self.accelerator.device)

        # Convert to DeepSeek-OCR tuple format: [(crop_batch, ori_batch)]
        # This is a LIST with ONE TUPLE containing the full batched tensors
        images_tuples = [(images_crop, images_ori)]

        # Encode instructions if text encoder is enabled
        text_embeddings = None
        if self.text_encoder is not None and instructions is not None:
            # Encode instructions to 4096-dim embeddings
            text_embeddings = self.text_encoder.encode(
                instructions,
                max_length=128,  # Instructions are typically short
            )
            # text_embeddings shape: (batch_size, 4096)

        # Forward pass with DeepSeek-OCR format (following infer() pattern)
        forward_kwargs = {
            "images": images_tuples,  # List of (crop, ori) tuples
            "images_spatial_crop": images_spatial_crop,  # [[1, 1], [1, 1], ...]
            "images_seq_mask": images_seq_mask,  # Boolean mask for image tokens
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": labels,
            "return_dict": True,
        }

        # Add text embeddings if available
        if text_embeddings is not None:
            forward_kwargs["text_embeddings"] = text_embeddings

        outputs = self.model(**forward_kwargs)

        # Extract loss
        loss = outputs.loss if hasattr(outputs, "loss") else outputs[0]

        return loss

    def train(
        self,
        dataset: Union[Iterable[dict], list, Dataset],
        eval_dataset: Optional[Union[Iterable[dict], list, Dataset]] = None,
        progress_callback: Optional[callable] = None,
    ) -> Tuple[Dict[str, Any], Dict[str, str]]:
        """
        Train the model.

        Args:
            dataset: Training dataset (iterable, list, or Dataset)
            eval_dataset: Optional evaluation dataset
            progress_callback: Optional callback(current_samples, total_samples)

        Returns:
            Tuple of (metrics, checkpoints)
        """
        # Create image transform pipeline for training
        # DeepSeek-OCR expects PIL images, not tensors - it handles its own processing
        if self.config.use_augmentation:
            LOGGER.info("Creating training transform with augmentation")
            transform = create_training_transform(
                resolution=self.config.target_resolution,
                use_augmentation=True,
                rotation_degrees=self.config.rotation_degrees,
                perspective_distortion=self.config.perspective_distortion,
                perspective_prob=self.config.perspective_prob,
                color_jitter_brightness=self.config.color_jitter_brightness,
                color_jitter_contrast=self.config.color_jitter_contrast,
                horizontal_flip_prob=self.config.horizontal_flip_prob,
                to_tensor=False,  # Keep as PIL images for DeepSeek-OCR
                normalize=False,
            )
        else:
            LOGGER.info("Creating training transform without augmentation")
            transform = create_training_transform(
                resolution=self.config.target_resolution,
                use_augmentation=False,
                to_tensor=False,  # Keep as PIL images for DeepSeek-OCR
                normalize=False,
            )

        # Wrap dataset with DeepSynthDataset to apply transform
        # Always wrap if not already a DeepSynthDataset (including HuggingFace datasets)
        if not isinstance(dataset, DeepSynthDataset):
            dataset = DeepSynthDataset(dataset, transform=transform)

        # Create DataLoader
        train_loader = self._create_dataloader(dataset, shuffle=True)

        # Create learning rate scheduler
        num_training_steps = len(train_loader) * self.config.num_epochs
        num_warmup_steps = self.config.optimizer.warmup_steps

        scheduler = get_scheduler(
            "cosine",
            optimizer=self.optimizer,
            num_warmup_steps=num_warmup_steps,
            num_training_steps=num_training_steps,
        )

        # Prepare for distributed training
        self.model, self.optimizer, train_loader, scheduler = self.accelerator.prepare(
            self.model, self.optimizer, train_loader, scheduler
        )

        LOGGER.info("Starting training: %d samples, %d epochs", len(dataset), self.config.num_epochs)
        LOGGER.info("Total training steps: %d, Warmup steps: %d", num_training_steps, num_warmup_steps)

        self.model.train()
        total_samples = len(dataset)
        processed_samples = 0

        for epoch in range(self.config.num_epochs):
            LOGGER.info("=" * 80)
            LOGGER.info("Epoch %d/%d", epoch + 1, self.config.num_epochs)
            LOGGER.info("=" * 80)

            epoch_loss = 0.0
            epoch_start_time = time.time()

            pbar = tqdm(
                train_loader,
                desc=f"Epoch {epoch + 1}/{self.config.num_epochs}",
                disable=not self.accelerator.is_local_main_process,
            )

            for step, batch in enumerate(pbar):
                with self.accelerator.accumulate(self.model):
                    # Forward pass
                    loss = self._forward_step(batch)

                    # Backward pass
                    self.accelerator.backward(loss)

                    # Apply gradient dropout if configured
                    if self.accelerator.sync_gradients:
                        self._apply_gradient_dropout()

                    # Optimizer step
                    self.optimizer.step()
                    scheduler.step()
                    self.optimizer.zero_grad()

                # Track metrics
                loss_value = loss.detach().float().item()
                epoch_loss += loss_value
                self.global_step += 1
                processed_samples += self.config.batch_size

                # Update progress bar
                pbar.set_postfix({
                    "loss": f"{loss_value:.4f}",
                    "lr": f"{scheduler.get_last_lr()[0]:.2e}",
                })

                # Progress callback
                if progress_callback and self.accelerator.is_local_main_process:
                    progress_callback(processed_samples, total_samples * self.config.num_epochs)

                # Save checkpoint at intervals
                if (self.global_step % self.config.save_interval == 0 and
                    self.accelerator.is_local_main_process):
                    checkpoint_dir = self.output_dir / f"checkpoint-{self.global_step}"
                    self.save_checkpoint(checkpoint_dir)

            # Epoch complete
            avg_epoch_loss = epoch_loss / len(train_loader)
            self.train_losses.append(avg_epoch_loss)
            epoch_time = time.time() - epoch_start_time
            samples_per_sec = total_samples / epoch_time

            LOGGER.info("Epoch %d complete: avg_loss=%.4f, time=%.1fs, samples/sec=%.1f",
                       epoch + 1, avg_epoch_loss, epoch_time, samples_per_sec)

            # Save epoch checkpoint
            if self.accelerator.is_local_main_process:
                checkpoint_dir = self.output_dir / f"epoch_{epoch + 1}"
                self.save_checkpoint(checkpoint_dir)

                # Push to Hub if configured
                if (self.config.save_checkpoints_to_hub and
                    self.config.push_to_hub and
                    self.config.hub_model_id):
                    self.push_checkpoint_to_hub(checkpoint_dir)

        # Training complete
        LOGGER.info("=" * 80)
        LOGGER.info("Training Complete!")
        LOGGER.info("=" * 80)

        # Prepare metrics
        metrics = {
            "train_loss": self.train_losses[-1] if self.train_losses else 0.0,
            "train_loss_per_epoch": self.train_losses,
            "epochs": self.config.num_epochs,
            "total_steps": self.global_step,
        }

        # Evaluation
        if eval_dataset:
            eval_loss = self.evaluate(eval_dataset)
            metrics["eval_loss"] = eval_loss
            metrics["eval_perplexity"] = math.exp(eval_loss) if eval_loss < 50 else float("inf")

        # Save final model
        if self.accelerator.is_local_main_process:
            self.save_checkpoint(self.output_dir)

            # Save metrics
            metrics_path = self.output_dir / "metrics.json"
            with open(metrics_path, "w") as f:
                json.dump({
                    "config": asdict(self.config),
                    "metrics": metrics,
                }, f, indent=2)

            # Push to Hub
            if self.config.push_to_hub and self.config.hub_model_id:
                self.push_to_hub(self.config.hub_model_id)

        LOGGER.info("Artifacts saved to: %s", self.output_dir)

        return metrics, {"last_checkpoint": str(self.output_dir)}

    def evaluate(self, dataset: Union[Iterable[dict], list, Dataset]) -> float:
        """Evaluate model on dataset."""
        LOGGER.info("Running evaluation...")

        # Create transform without augmentation
        transform = create_inference_transform(resolution=self.config.target_resolution)

        if not isinstance(dataset, Dataset):
            dataset = DeepSynthDataset(dataset, transform=transform)

        eval_loader = self._create_dataloader(dataset, shuffle=False)

        self.model.eval()
        total_loss = 0.0
        num_batches = 0

        with torch.no_grad():
            for batch in tqdm(eval_loader, desc="Evaluating", disable=not self.accelerator.is_local_main_process):
                loss = self._forward_step(batch)
                total_loss += loss.item()
                num_batches += 1

        self.model.train()
        avg_loss = total_loss / num_batches if num_batches > 0 else 0.0

        LOGGER.info("Evaluation loss: %.4f", avg_loss)
        return avg_loss

    def _apply_gradient_dropout(self) -> None:
        """Apply configured dropout controllers on accumulated gradients."""
        if self._expert_dropout is not None:
            self._expert_dropout.apply()
        if self._gate_dropout is not None:
            self._gate_dropout.apply()

    def save_checkpoint(self, output_dir: Union[str, Path]) -> None:
        """Save model, tokenizer, and training state."""
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        LOGGER.info("Saving checkpoint to: %s", output_dir)

        # Unwrap model for saving
        unwrapped_model = self.accelerator.unwrap_model(self.model)

        # Save model and tokenizer
        unwrapped_model.save_pretrained(
            output_dir,
            is_main_process=self.accelerator.is_main_process,
            save_function=self.accelerator.save,
        )

        if self.accelerator.is_main_process:
            self.tokenizer.save_pretrained(output_dir)

            # Save training config
            config_path = output_dir / "training_config.json"
            with open(config_path, "w") as f:
                json.dump(asdict(self.config), f, indent=2)

            LOGGER.info("✓ Checkpoint saved successfully")

    def push_checkpoint_to_hub(self, checkpoint_dir: Path) -> None:
        """Push checkpoint to HuggingFace Hub."""
        if not self.config.hub_model_id:
            return

        try:
            LOGGER.info("Pushing checkpoint to Hub: %s", self.config.hub_model_id)
            self.api.upload_folder(
                folder_path=str(checkpoint_dir),
                repo_id=self.config.hub_model_id,
                repo_type="model",
                token=self.config.hub_token,
            )
            LOGGER.info("✓ Checkpoint pushed to Hub")
        except Exception as e:
            LOGGER.error("Failed to push checkpoint to Hub: %s", e)

    def push_to_hub(self, repo_id: str) -> None:
        """Push final model to HuggingFace Hub."""
        try:
            LOGGER.info("Pushing model to Hub: %s", repo_id)

            # Create repo if needed
            try:
                create_repo(
                    repo_id=repo_id,
                    repo_type="model",
                    private=self.config.hub_private,
                    token=self.config.hub_token,
                    exist_ok=True,
                )
            except Exception as e:
                LOGGER.warning("Repo creation warning: %s", e)

            # Upload
            self.api.upload_folder(
                folder_path=str(self.output_dir),
                repo_id=repo_id,
                repo_type="model",
                token=self.config.hub_token,
            )

            LOGGER.info("✓ Model pushed to Hub: https://huggingface.co/%s", repo_id)
        except Exception as e:
            LOGGER.error("Failed to push to Hub: %s", e)

    def train_from_hf_dataset(
        self,
        repo_id: str,
        progress_callback: Optional[callable] = None,
    ) -> Tuple[Dict[str, Any], Dict[str, str]]:
        """
        Load dataset from HuggingFace Hub and train.

        Args:
            repo_id: HuggingFace dataset repository ID
            progress_callback: Optional progress callback

        Returns:
            Tuple of (metrics, checkpoints)
        """
        LOGGER.info("Loading dataset from HuggingFace Hub: %s", repo_id)
        dataset_dict = load_dataset(repo_id)

        if not isinstance(dataset_dict, DatasetDict):
            raise ValueError("Expected a dataset with train split")

        train_split = dataset_dict["train"]
        eval_split = None

        if self.config.evaluation_split and self.config.evaluation_split in dataset_dict:
            eval_split = dataset_dict[self.config.evaluation_split]

        train_samples = train_split.to_list()
        eval_samples = eval_split.to_list() if eval_split else None

        return self.train(train_samples, eval_samples, progress_callback=progress_callback)


# Alias for backwards compatibility
ProductionDeepSynthTrainer = UnifiedProductionTrainer

__all__ = ["UnifiedProductionTrainer", "ProductionDeepSynthTrainer", "DeepSynthDataset"]
